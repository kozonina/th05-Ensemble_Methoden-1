{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methoden - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Methoden sind Meta-Algorithmen, die in der Statistik und für Machine Learning eingesetzt werden. Sie nutzen eine endliche Menge von verschiedenen Lernalgorithmen, um bessere Ergebnisse zu erhalten, als mit einem einzelnen Lernalgorithmus. \n",
    "\n",
    "Ziele der Ensemble Methoden sind: \n",
    "        - Bias zu minimieren       (z.B Boosting)\n",
    "        - Varianz zu minimieren    (z.B Bagging) \n",
    "        - Vorhersagen zu verbessern (z.B Stacking)\n",
    "        \n",
    "Ensemble-Methoden können in zwei Gruppen unterteilt werden:\n",
    " \n",
    " 1. **sequentielle** Ensemble Methoden, bei denen die Basis-Klassifikatoren nacheinander erzeugt werden (z. B. AdaBoost).\n",
    " \n",
    " 2. **parallele** Ensemble Methoden, bei denen die Basis-Klassifikatoren parallel erzeugt werden (z. B. Random Forest).\n",
    "\n",
    "Ein wichtiges Einsatzgebiet von Ensemble - Methoden sind Entscheidungsbäume. Ein großer Entscheidungsbaum neigt zu hohen Fehlerraten und einer hohen Varianz, da von der Wurzel zu den Blättern viele Entscheidungsknoten liegen, die alle unter Unsicherheit durchlaufen werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting (Verstärken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es ist ein Algorithmus der automatischen Klassifizierung, der mehrere schwache Klassifikatoren zu einem einzigen guten, möglichst fehlerfreien Klassifikator verschmilzt. Boosting liefert akzeptable Ergebnisse und lässt sich einfach in ein Computerprogramm umsetzen, das sparsam im Speicherbedarf und schnell in der Laufzeit ist.\n",
    "\n",
    "Boosting verringert den Bias, erhöht aber leicht die Varianz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel Algorithmus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost Algorithmus\n",
    "Erklärung zu folgendem Bild: \n",
    "Der erste Klassifikator trainiert auf ungewichteten Daten, dann gewichtet er die Daten für den nächsten Klassifikator und so weiter, um den endgültigen Klassifizierer zu erzeugen. \n",
    "\n",
    "![Boosting](img/Boosting.png) \n",
    "\n",
    "**Quelle :** https://www.researchgate.net/figure/Training-of-an-AdaBoost-classifier-The-first-classifier-trains-on-unweighted-data-then_fig3_306054843"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vorteile** <br>\n",
    "\n",
    "Relativ wenig Arbeit und ein sehr gutes Ergebnis. <br>\n",
    "Kann problemlos mit qualitativen (kategorischen) Merkmalen umgehen. <br>\n",
    "\n",
    "**Nachteile** \n",
    "\n",
    "Es kann zu ein \"Overfitting\" führen, wenn die Anzahl von den Entscheidungsbäumen zu groß ist.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging steht für Bootstrap-Aggregating und wurde von Leo Breiman entwickelt.\n",
    "Beim Bagging werden aus dem Trainingsdatensatz (Datensatz von originalen Daten) mehrere Bootstrap-Stichproben generiert. Unter einer Boostrap-Stichprobe versteht man eine Stichprobe der gleichen Größe wie die Ursprungsmenge, die durch zufälliges Ziehen mit Zurücklegen aus der Ursprungsmenge erzeugt wird. Manche Beispiele werden dadurch mehrfach gewählt, andere gar nicht. Im Schnitt kommen in einer Bootstrap-Stichprobe 63,2 % der Beispiele aus der Ursprungsmenge vor. Die jeweils nicht gewählten heißen Out-of-bag-Beispiele (OOB-Beispiele). Auf jeder der Bootstrap-Stichproben wird ein Basis-Klassifikator \"trainiert\". Die abschließende Klassifikation der Daten erfolgt dann per Mehrheitsentscheid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bagging](img/bagging.jpg)\n",
    "\n",
    "**Quelle : **Ensemble Methoden zur automatisierten Klassifikation von NMR-Spektren\n",
    "Diplomarbeit von Jana Ehlers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch das Erzeugen der Bootstrap-Stichproben wird die Korrelation (Zusammenhang, bei dem die genannten Elemente wechselseitig aufeinander wirken) der Klassifikatoren untereinander gering gehalten. Das Mittel über viele Basis-Klassifikatoren reduziert außerdem die Varianz und verbessert damit die Klassifikationsleistung.\n",
    "Der Bias bleibt ungefähr gleich. Bei Breiman bewegten sich die Verbesserungen\n",
    "der Korrektklassifikationsrate zwischen 20-47%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vorteile**\n",
    "\n",
    "Kann problemlos mit qualitativen (kategorischen) Merkmalen umgehen.<br>\n",
    "Out of bag (OOB) Schätzungen können für die Modellvalidierung verwendet werden.\n",
    "\n",
    "**Nachteile**\n",
    "\n",
    "Nicht so einfach zu interpretieren. <br>\n",
    "Reduziert die Varianz nicht, wenn die Merkmale korreliert sind.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking versucht die Vorteile aller bekannten Lernalgorithmen zu vereinen. Diese Methode kombiniert mehrere Klassifikations- oder Regressionsmodelle über einen Meta-Klassifikator oder einen Meta-Regressor. <br>\n",
    "Die erste Modelle werden von einem vollständingen Trainingssdatensatz \"trainiert\" und geben jeweils ein Klassifikator aus. Dann wird ein neuer Datensatz von Prädiktoren erstellt, der als Trainingsdatensatz für die nächste Ebene von Modellen (in diesem Fall 1 Modell) benutzt wird. Sie geben einen kombinierten Klassifikator aus und zum Schluss eine sehr starke Vorhersage.\n",
    "\n",
    "\n",
    "Stacking ist insbesondere dann sinnvoll, wenn die Ergebnisse der einzelnen Algorithmen sehr unterschiedlich ausfallen können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stacking](img/stacking.jpg)\n",
    "\n",
    "**Quelle :** Advanced Machine Learning with Python Solve challenging data science problems by mastering cutting-edge machine learning techniques in Python John Hearty Kapitel 8 Ensemble Methods S.216"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Random - Forest - Algorithmus gehört zur Klasse der Bagging - Algorithmen. Ein Random Forest ist ein Klassifikationsverfahren, das aus mehreren unkorrelierten Entscheidungsbäumen besteht. Alle Entscheidungsbäume sind unter einer bestimmten Art von Randomisierung während des Lernprozesses gewachsen. Für eine Klassifikation darf jeder Baum in diesem Wald eine Entscheidung treffen und die Klasse mit den meisten Stimmen entscheidet die endgültige Klassifikation. Random Forests können auch zur Regression eingesetzt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Klassifikator lernt sehr schnell: Dieser Vorteil ergibt sich durch die kurze Trainings- bzw. Aufbauzeit eines einzelnen Entscheidungsbaumes und dadurch, dass die Trainingszeit bei einem Random Forest linear mit der Anzahl der Bäume steigt.\n",
    "<br> Die Evaluierung eines Testbeispieles geschieht auf jedem Baum einzeln, ist parallelisierbar und daher zeitsparend. \n",
    "<br> Random Forest ist sehr effizient für große Datenmengen (viele Klassen, viele Trainingsbeispiele, viele Merkmale).\n",
    "Wichtige Klassen und der Zusammenhang zwischen den Klassen kann erkannt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wichtig:**  Random Forest ist sehr speziell, da es bei jedem Knoten bzg. Baum zufällig k Variablen aus verfügbaren Variablen N auswählt. Die Anzahl der k Variablen ist normalerweise viel kleiner als die Gesamtzahl der Variablen N. (k < N) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Beispiel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beschreibung:** Man hat 100 Männer. Jeder geht in eine Kneipe und möchte eine Frau ansprechen. Jeder möchte vorher wissen, ob er Erfolg hat oder ob er einen Korb bekommt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RandomForest](img/RandomForest.jpg)\n",
    "\n",
    "**Quelle :** Einfache Einführung in den Random Forest Algorithmus zur Klassifikation https://www.youtube.com/watch?v=gksq_QaJ34w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'randomForest' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\Kristina\\AppData\\Local\\Temp\\RtmpkROsNP\\downloaded_packages\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"randomForest\", repos=\"http://cran.r-project.org\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'randomForest' was built under R version 3.4.4\"randomForest 4.6-14\n",
      "Type rfNews() to see new features/changes/bug fixes.\n"
     ]
    }
   ],
   "source": [
    "library(\"randomForest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetTrain <- read.table(\"C:/Users/Kristina/Desktop/Uni/5.Semester/Wahlfächer/Data Science mit Python und R/Random Forest/data/exampleDatasetTrain.csv\", sep = \",\", header=TRUE)\n",
    "datasetTest <- read.table(\"C:/Users/Kristina/Desktop/Uni/5.Semester/Wahlfächer/Data Science mit Python und R/Random Forest/data/exampleDatasetTest.csv\", sep = \",\", header=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwei Datensätze laden, die jeweils 100 unterschiedliche Männer enthalten. Der Datensatz \"datasetTrain\" wird verwentet, um \"Random Forests\" zu generieren. Der Datensatz \"datasetTest\" wird später verwendet, um zu sehen, wie das erzeugte Modell (Random Forest) auch mit anderen neuen Beobachtungen funktioniert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t100 obs. of  7 variables:\n",
      " $ altersunterschied      : int  -1 2 2 -1 2 2 0 7 1 2 ...\n",
      " $ iq                     : int  85 83 86 109 75 101 86 83 93 83 ...\n",
      " $ konsumierte_bierglaeser: int  5 3 1 2 3 0 1 6 4 5 ...\n",
      " $ aufenthaltsdauer       : num  180.1 66.5 29.9 77.1 117.8 ...\n",
      " $ haarfarbe              : Factor w/ 4 levels \"blond\",\"braun\",..: 3 3 2 1 1 4 2 4 2 3 ...\n",
      " $ hat_Freundin           : logi  TRUE FALSE FALSE TRUE FALSE FALSE ...\n",
      " $ ergebnis               : Factor w/ 2 levels \"Erfolg\",\"Korb\": 2 2 1 1 2 1 2 2 2 2 ...\n",
      "'data.frame':\t100 obs. of  7 variables:\n",
      " $ altersunterschied      : int  -7 2 1 2 -1 -1 -4 -2 0 -6 ...\n",
      " $ iq                     : int  91 92 106 105 71 70 73 85 98 71 ...\n",
      " $ konsumierte_bierglaeser: int  2 4 6 0 5 3 6 5 2 3 ...\n",
      " $ aufenthaltsdauer       : num  69.6 126.9 151.5 0 165.3 ...\n",
      " $ haarfarbe              : Factor w/ 4 levels \"blond\",\"braun\",..: 4 4 2 2 3 1 3 3 2 2 ...\n",
      " $ hat_Freundin           : logi  TRUE TRUE TRUE TRUE FALSE FALSE ...\n",
      " $ ergebnis               : Factor w/ 2 levels \"Erfolg\",\"Korb\": 1 1 2 1 2 2 2 2 1 2 ...\n"
     ]
    }
   ],
   "source": [
    "str(datasetTrain)\n",
    "str(datasetTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier geben wir den Aufbau der Datensätze aus. Jeder Datensatz hat 100 Beobachtungen und 7 Variablen. Die Variable \"Ergebnis\" muss von Datentyp \"Factor\" sein, sonst bekommt man eine Fehlermeldung. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>altersunterschied</th><th scope=col>iq</th><th scope=col>konsumierte_bierglaeser</th><th scope=col>aufenthaltsdauer</th><th scope=col>haarfarbe</th><th scope=col>hat_Freundin</th><th scope=col>ergebnis</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>-1       </td><td> 85      </td><td>5        </td><td>180.09167</td><td>rot      </td><td> TRUE    </td><td>Korb     </td></tr>\n",
       "\t<tr><td> 2       </td><td> 83      </td><td>3        </td><td> 66.47255</td><td>rot      </td><td>FALSE    </td><td>Korb     </td></tr>\n",
       "\t<tr><td> 2       </td><td> 86      </td><td>1        </td><td> 29.93965</td><td>braun    </td><td>FALSE    </td><td>Erfolg   </td></tr>\n",
       "\t<tr><td>-1       </td><td>109      </td><td>2        </td><td> 77.08777</td><td>blond    </td><td> TRUE    </td><td>Erfolg   </td></tr>\n",
       "\t<tr><td> 2       </td><td> 75      </td><td>3        </td><td>117.83805</td><td>blond    </td><td>FALSE    </td><td>Korb     </td></tr>\n",
       "\t<tr><td> 2       </td><td>101      </td><td>0        </td><td>  0.00000</td><td>schwarz  </td><td>FALSE    </td><td>Erfolg   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllll}\n",
       " altersunterschied & iq & konsumierte\\_bierglaeser & aufenthaltsdauer & haarfarbe & hat\\_Freundin & ergebnis\\\\\n",
       "\\hline\n",
       "\t -1        &  85       & 5         & 180.09167 & rot       &  TRUE     & Korb     \\\\\n",
       "\t  2        &  83       & 3         &  66.47255 & rot       & FALSE     & Korb     \\\\\n",
       "\t  2        &  86       & 1         &  29.93965 & braun     & FALSE     & Erfolg   \\\\\n",
       "\t -1        & 109       & 2         &  77.08777 & blond     &  TRUE     & Erfolg   \\\\\n",
       "\t  2        &  75       & 3         & 117.83805 & blond     & FALSE     & Korb     \\\\\n",
       "\t  2        & 101       & 0         &   0.00000 & schwarz   & FALSE     & Erfolg   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "altersunterschied | iq | konsumierte_bierglaeser | aufenthaltsdauer | haarfarbe | hat_Freundin | ergebnis | \n",
       "|---|---|---|---|---|---|\n",
       "| -1        |  85       | 5         | 180.09167 | rot       |  TRUE     | Korb      | \n",
       "|  2        |  83       | 3         |  66.47255 | rot       | FALSE     | Korb      | \n",
       "|  2        |  86       | 1         |  29.93965 | braun     | FALSE     | Erfolg    | \n",
       "| -1        | 109       | 2         |  77.08777 | blond     |  TRUE     | Erfolg    | \n",
       "|  2        |  75       | 3         | 117.83805 | blond     | FALSE     | Korb      | \n",
       "|  2        | 101       | 0         |   0.00000 | schwarz   | FALSE     | Erfolg    | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  altersunterschied iq  konsumierte_bierglaeser aufenthaltsdauer haarfarbe\n",
       "1 -1                 85 5                       180.09167        rot      \n",
       "2  2                 83 3                        66.47255        rot      \n",
       "3  2                 86 1                        29.93965        braun    \n",
       "4 -1                109 2                        77.08777        blond    \n",
       "5  2                 75 3                       117.83805        blond    \n",
       "6  2                101 0                         0.00000        schwarz  \n",
       "  hat_Freundin ergebnis\n",
       "1  TRUE        Korb    \n",
       "2 FALSE        Korb    \n",
       "3 FALSE        Erfolg  \n",
       "4  TRUE        Erfolg  \n",
       "5 FALSE        Korb    \n",
       "6 FALSE        Erfolg  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(datasetTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die erste 6 Beobactungen des \"datasetTrain\" anzeigen lassen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Algorithmus \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntree      OOB      1      2\n",
      "    1:  16.22% 15.38% 16.67%\n",
      "    2:  16.13% 15.00% 16.67%\n",
      "    3:  17.11% 12.00% 19.61%\n",
      "    4:  18.39% 15.62% 20.00%\n",
      "    5:  18.68% 12.12% 22.41%\n",
      "    6:  24.18% 30.30% 20.69%\n",
      "    7:  21.05% 16.67% 23.73%\n",
      "    8:  20.41% 15.38% 23.73%\n",
      "    9:  18.18% 22.50% 15.25%\n",
      "   10:  19.19% 22.50% 16.95%\n",
      "   11:  19.19% 22.50% 16.95%\n",
      "   12:  18.18% 20.00% 16.95%\n",
      "   13:  18.18% 22.50% 15.25%\n",
      "   14:  18.18% 22.50% 15.25%\n",
      "   15:  18.00% 22.50% 15.00%\n",
      "   16:  17.00% 20.00% 15.00%\n",
      "   17:  17.00% 20.00% 15.00%\n",
      "   18:  17.00% 17.50% 16.67%\n",
      "   19:  16.00% 17.50% 15.00%\n",
      "   20:  17.00% 20.00% 15.00%\n",
      "   21:  17.00% 17.50% 16.67%\n",
      "   22:  18.00% 20.00% 16.67%\n",
      "   23:  18.00% 17.50% 18.33%\n",
      "   24:  18.00% 17.50% 18.33%\n",
      "   25:  19.00% 20.00% 18.33%\n",
      "   26:  20.00% 22.50% 18.33%\n",
      "   27:  16.00% 17.50% 15.00%\n",
      "   28:  18.00% 17.50% 18.33%\n",
      "   29:  18.00% 20.00% 16.67%\n",
      "   30:  19.00% 22.50% 16.67%\n",
      "   31:  17.00% 17.50% 16.67%\n",
      "   32:  17.00% 20.00% 15.00%\n",
      "   33:  18.00% 20.00% 16.67%\n",
      "   34:  17.00% 20.00% 15.00%\n",
      "   35:  18.00% 22.50% 15.00%\n",
      "   36:  18.00% 22.50% 15.00%\n",
      "   37:  18.00% 22.50% 15.00%\n",
      "   38:  19.00% 25.00% 15.00%\n",
      "   39:  17.00% 20.00% 15.00%\n",
      "   40:  18.00% 22.50% 15.00%\n",
      "   41:  17.00% 20.00% 15.00%\n",
      "   42:  17.00% 20.00% 15.00%\n",
      "   43:  17.00% 20.00% 15.00%\n",
      "   44:  18.00% 22.50% 15.00%\n",
      "   45:  18.00% 22.50% 15.00%\n",
      "   46:  17.00% 20.00% 15.00%\n",
      "   47:  19.00% 25.00% 15.00%\n",
      "   48:  18.00% 22.50% 15.00%\n",
      "   49:  17.00% 20.00% 15.00%\n",
      "   50:  18.00% 22.50% 15.00%\n",
      "   51:  18.00% 22.50% 15.00%\n",
      "   52:  18.00% 22.50% 15.00%\n",
      "   53:  17.00% 20.00% 15.00%\n",
      "   54:  17.00% 20.00% 15.00%\n",
      "   55:  17.00% 20.00% 15.00%\n",
      "   56:  18.00% 22.50% 15.00%\n",
      "   57:  17.00% 20.00% 15.00%\n",
      "   58:  17.00% 20.00% 15.00%\n",
      "   59:  17.00% 20.00% 15.00%\n",
      "   60:  17.00% 20.00% 15.00%\n",
      "   61:  17.00% 20.00% 15.00%\n",
      "   62:  18.00% 22.50% 15.00%\n",
      "   63:  17.00% 20.00% 15.00%\n",
      "   64:  17.00% 20.00% 15.00%\n",
      "   65:  18.00% 22.50% 15.00%\n",
      "   66:  17.00% 20.00% 15.00%\n",
      "   67:  18.00% 22.50% 15.00%\n",
      "   68:  17.00% 20.00% 15.00%\n",
      "   69:  17.00% 20.00% 15.00%\n",
      "   70:  18.00% 22.50% 15.00%\n",
      "   71:  17.00% 20.00% 15.00%\n",
      "   72:  17.00% 20.00% 15.00%\n",
      "   73:  18.00% 22.50% 15.00%\n",
      "   74:  19.00% 25.00% 15.00%\n",
      "   75:  17.00% 20.00% 15.00%\n",
      "   76:  18.00% 22.50% 15.00%\n",
      "   77:  19.00% 25.00% 15.00%\n",
      "   78:  20.00% 27.50% 15.00%\n",
      "   79:  19.00% 25.00% 15.00%\n",
      "   80:  19.00% 25.00% 15.00%\n",
      "   81:  19.00% 25.00% 15.00%\n",
      "   82:  19.00% 25.00% 15.00%\n",
      "   83:  19.00% 25.00% 15.00%\n",
      "   84:  19.00% 25.00% 15.00%\n",
      "   85:  18.00% 22.50% 15.00%\n",
      "   86:  18.00% 22.50% 15.00%\n",
      "   87:  18.00% 22.50% 15.00%\n",
      "   88:  19.00% 25.00% 15.00%\n",
      "   89:  19.00% 25.00% 15.00%\n",
      "   90:  18.00% 22.50% 15.00%\n",
      "   91:  19.00% 25.00% 15.00%\n",
      "   92:  18.00% 22.50% 15.00%\n",
      "   93:  18.00% 22.50% 15.00%\n",
      "   94:  17.00% 20.00% 15.00%\n",
      "   95:  18.00% 22.50% 15.00%\n",
      "   96:  18.00% 22.50% 15.00%\n",
      "   97:  18.00% 22.50% 15.00%\n",
      "   98:  18.00% 22.50% 15.00%\n",
      "   99:  18.00% 22.50% 15.00%\n",
      "  100:  18.00% 22.50% 15.00%\n"
     ]
    }
   ],
   "source": [
    "set.seed(123)\n",
    "myRF <- randomForest(ergebnis ~ ., data=datasetTrain, importance=TRUE, mtry=2,\n",
    "                     ntree=100, replace=TRUE, do.trace=TRUE, keep.forest=TRUE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**set.seed(123):** Sicher stellen, dass alle den gleichen Random Forest generieren \n",
    "\n",
    "**ergebnis:** Variable die der Random Forest vorhersagen soll <br>\n",
    "**~. :** stellt sicher das alle andere Variablen als Ptrdiktoren genutzt werden ,also basierend auf den man das ergebnis vorher sagt.<br>\n",
    "**data:** als Daten wird der Trainingsdatensatz verwendet<br>\n",
    "**Importance:** Lässt danach die Wichtigkeiten der Variablen berechnen<br>\n",
    "**mtry:** Wie viele Variablen sollen bei jedem Baum zufällig gezogen werden und betrachtet werden für den Split?<br>\n",
    "**ntree:** Wie viele Bäume soll mein Forest beinhalten?<br>\n",
    "**replace:** Mit zurücklegen<br>\n",
    "**do.trace:** Output anzeigen beim Erstellen des Waldes<br>\n",
    "**keep.forest:** Man kann die einzelne Bäume später betrachten<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       " randomForest(formula = ergebnis ~ ., data = datasetTrain, importance = TRUE,      mtry = 2, ntree = 100, replace = TRUE, do.trace = TRUE, keep.forest = TRUE) \n",
       "               Type of random forest: classification\n",
       "                     Number of trees: 100\n",
       "No. of variables tried at each split: 2\n",
       "\n",
       "        OOB estimate of  error rate: 18%\n",
       "Confusion matrix:\n",
       "       Erfolg Korb class.error\n",
       "Erfolg     31    9       0.225\n",
       "Korb        9   51       0.150"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird angezeigt, dass man eine Klassifikation mit 100 Bäumen hat mit jeweils zwei zufällig ausgewählten Variablen für jeden Baum.\n",
    "\n",
    "Der OOB-Fehler liegt bei 18% . Das heißt, 82% der Männer wurden richtig klassifiziert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>left daughter</th><th scope=col>right daughter</th><th scope=col>split var</th><th scope=col>split point</th><th scope=col>status</th><th scope=col>prediction</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 2               </td><td> 3               </td><td>aufenthaltsdauer </td><td> 42.52578        </td><td> 1               </td><td>NA               </td></tr>\n",
       "\t<tr><td> 4               </td><td> 5               </td><td>iq               </td><td> 77.50000        </td><td> 1               </td><td>NA               </td></tr>\n",
       "\t<tr><td> 6               </td><td> 7               </td><td>aufenthaltsdauer </td><td>113.47529        </td><td> 1               </td><td>NA               </td></tr>\n",
       "\t<tr><td> 0               </td><td> 0               </td><td>NA               </td><td>  0.00000        </td><td>-1               </td><td>Korb             </td></tr>\n",
       "\t<tr><td> 8               </td><td> 9               </td><td>iq               </td><td> 92.00000        </td><td> 1               </td><td>NA               </td></tr>\n",
       "\t<tr><td>10               </td><td>11               </td><td>iq               </td><td> 88.50000        </td><td> 1               </td><td>NA               </td></tr>\n",
       "\t<tr><td> 0               </td><td> 0               </td><td>NA               </td><td>  0.00000        </td><td>-1               </td><td>Korb             </td></tr>\n",
       "\t<tr><td>12               </td><td>13               </td><td>aufenthaltsdauer </td><td> 21.83633        </td><td> 1               </td><td>NA               </td></tr>\n",
       "\t<tr><td> 0               </td><td> 0               </td><td>NA               </td><td>  0.00000        </td><td>-1               </td><td>Erfolg           </td></tr>\n",
       "\t<tr><td> 0               </td><td> 0               </td><td>NA               </td><td>  0.00000        </td><td>-1               </td><td>Korb             </td></tr>\n",
       "\t<tr><td>14               </td><td>15               </td><td>aufenthaltsdauer </td><td>109.67921        </td><td> 1               </td><td>NA               </td></tr>\n",
       "\t<tr><td> 0               </td><td> 0               </td><td>NA               </td><td>  0.00000        </td><td>-1               </td><td>Erfolg           </td></tr>\n",
       "\t<tr><td>16               </td><td>17               </td><td>iq               </td><td> 88.50000        </td><td> 1               </td><td>NA               </td></tr>\n",
       "\t<tr><td>18               </td><td>19               </td><td>altersunterschied</td><td>  1.00000        </td><td> 1               </td><td>NA               </td></tr>\n",
       "\t<tr><td> 0               </td><td> 0               </td><td>NA               </td><td>  0.00000        </td><td>-1               </td><td>Erfolg           </td></tr>\n",
       "\t<tr><td>20               </td><td>21               </td><td>aufenthaltsdauer </td><td> 29.38046        </td><td> 1               </td><td>NA               </td></tr>\n",
       "\t<tr><td> 0               </td><td> 0               </td><td>NA               </td><td>  0.00000        </td><td>-1               </td><td>Korb             </td></tr>\n",
       "\t<tr><td>22               </td><td>23               </td><td>altersunterschied</td><td> -1.50000        </td><td> 1               </td><td>NA               </td></tr>\n",
       "\t<tr><td> 0               </td><td> 0               </td><td>NA               </td><td>  0.00000        </td><td>-1               </td><td>Erfolg           </td></tr>\n",
       "\t<tr><td> 0               </td><td> 0               </td><td>NA               </td><td>  0.00000        </td><td>-1               </td><td>Korb             </td></tr>\n",
       "\t<tr><td> 0               </td><td> 0               </td><td>NA               </td><td>  0.00000        </td><td>-1               </td><td>Erfolg           </td></tr>\n",
       "\t<tr><td> 0               </td><td> 0               </td><td>NA               </td><td>  0.00000        </td><td>-1               </td><td>Korb             </td></tr>\n",
       "\t<tr><td>24               </td><td>25               </td><td>haarfarbe        </td><td>  1.00000        </td><td> 1               </td><td>NA               </td></tr>\n",
       "\t<tr><td> 0               </td><td> 0               </td><td>NA               </td><td>  0.00000        </td><td>-1               </td><td>Erfolg           </td></tr>\n",
       "\t<tr><td> 0               </td><td> 0               </td><td>NA               </td><td>  0.00000        </td><td>-1               </td><td>Korb             </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " left daughter & right daughter & split var & split point & status & prediction\\\\\n",
       "\\hline\n",
       "\t  2                &  3                & aufenthaltsdauer  &  42.52578         &  1                & NA               \\\\\n",
       "\t  4                &  5                & iq                &  77.50000         &  1                & NA               \\\\\n",
       "\t  6                &  7                & aufenthaltsdauer  & 113.47529         &  1                & NA               \\\\\n",
       "\t  0                &  0                & NA                &   0.00000         & -1                & Korb             \\\\\n",
       "\t  8                &  9                & iq                &  92.00000         &  1                & NA               \\\\\n",
       "\t 10                & 11                & iq                &  88.50000         &  1                & NA               \\\\\n",
       "\t  0                &  0                & NA                &   0.00000         & -1                & Korb             \\\\\n",
       "\t 12                & 13                & aufenthaltsdauer  &  21.83633         &  1                & NA               \\\\\n",
       "\t  0                &  0                & NA                &   0.00000         & -1                & Erfolg           \\\\\n",
       "\t  0                &  0                & NA                &   0.00000         & -1                & Korb             \\\\\n",
       "\t 14                & 15                & aufenthaltsdauer  & 109.67921         &  1                & NA               \\\\\n",
       "\t  0                &  0                & NA                &   0.00000         & -1                & Erfolg           \\\\\n",
       "\t 16                & 17                & iq                &  88.50000         &  1                & NA               \\\\\n",
       "\t 18                & 19                & altersunterschied &   1.00000         &  1                & NA               \\\\\n",
       "\t  0                &  0                & NA                &   0.00000         & -1                & Erfolg           \\\\\n",
       "\t 20                & 21                & aufenthaltsdauer  &  29.38046         &  1                & NA               \\\\\n",
       "\t  0                &  0                & NA                &   0.00000         & -1                & Korb             \\\\\n",
       "\t 22                & 23                & altersunterschied &  -1.50000         &  1                & NA               \\\\\n",
       "\t  0                &  0                & NA                &   0.00000         & -1                & Erfolg           \\\\\n",
       "\t  0                &  0                & NA                &   0.00000         & -1                & Korb             \\\\\n",
       "\t  0                &  0                & NA                &   0.00000         & -1                & Erfolg           \\\\\n",
       "\t  0                &  0                & NA                &   0.00000         & -1                & Korb             \\\\\n",
       "\t 24                & 25                & haarfarbe         &   1.00000         &  1                & NA               \\\\\n",
       "\t  0                &  0                & NA                &   0.00000         & -1                & Erfolg           \\\\\n",
       "\t  0                &  0                & NA                &   0.00000         & -1                & Korb             \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "left daughter | right daughter | split var | split point | status | prediction | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "|  2                |  3                | aufenthaltsdauer  |  42.52578         |  1                | NA                | \n",
       "|  4                |  5                | iq                |  77.50000         |  1                | NA                | \n",
       "|  6                |  7                | aufenthaltsdauer  | 113.47529         |  1                | NA                | \n",
       "|  0                |  0                | NA                |   0.00000         | -1                | Korb              | \n",
       "|  8                |  9                | iq                |  92.00000         |  1                | NA                | \n",
       "| 10                | 11                | iq                |  88.50000         |  1                | NA                | \n",
       "|  0                |  0                | NA                |   0.00000         | -1                | Korb              | \n",
       "| 12                | 13                | aufenthaltsdauer  |  21.83633         |  1                | NA                | \n",
       "|  0                |  0                | NA                |   0.00000         | -1                | Erfolg            | \n",
       "|  0                |  0                | NA                |   0.00000         | -1                | Korb              | \n",
       "| 14                | 15                | aufenthaltsdauer  | 109.67921         |  1                | NA                | \n",
       "|  0                |  0                | NA                |   0.00000         | -1                | Erfolg            | \n",
       "| 16                | 17                | iq                |  88.50000         |  1                | NA                | \n",
       "| 18                | 19                | altersunterschied |   1.00000         |  1                | NA                | \n",
       "|  0                |  0                | NA                |   0.00000         | -1                | Erfolg            | \n",
       "| 20                | 21                | aufenthaltsdauer  |  29.38046         |  1                | NA                | \n",
       "|  0                |  0                | NA                |   0.00000         | -1                | Korb              | \n",
       "| 22                | 23                | altersunterschied |  -1.50000         |  1                | NA                | \n",
       "|  0                |  0                | NA                |   0.00000         | -1                | Erfolg            | \n",
       "|  0                |  0                | NA                |   0.00000         | -1                | Korb              | \n",
       "|  0                |  0                | NA                |   0.00000         | -1                | Erfolg            | \n",
       "|  0                |  0                | NA                |   0.00000         | -1                | Korb              | \n",
       "| 24                | 25                | haarfarbe         |   1.00000         |  1                | NA                | \n",
       "|  0                |  0                | NA                |   0.00000         | -1                | Erfolg            | \n",
       "|  0                |  0                | NA                |   0.00000         | -1                | Korb              | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   left daughter right daughter split var         split point status prediction\n",
       "1   2             3             aufenthaltsdauer   42.52578    1     NA        \n",
       "2   4             5             iq                 77.50000    1     NA        \n",
       "3   6             7             aufenthaltsdauer  113.47529    1     NA        \n",
       "4   0             0             NA                  0.00000   -1     Korb      \n",
       "5   8             9             iq                 92.00000    1     NA        \n",
       "6  10            11             iq                 88.50000    1     NA        \n",
       "7   0             0             NA                  0.00000   -1     Korb      \n",
       "8  12            13             aufenthaltsdauer   21.83633    1     NA        \n",
       "9   0             0             NA                  0.00000   -1     Erfolg    \n",
       "10  0             0             NA                  0.00000   -1     Korb      \n",
       "11 14            15             aufenthaltsdauer  109.67921    1     NA        \n",
       "12  0             0             NA                  0.00000   -1     Erfolg    \n",
       "13 16            17             iq                 88.50000    1     NA        \n",
       "14 18            19             altersunterschied   1.00000    1     NA        \n",
       "15  0             0             NA                  0.00000   -1     Erfolg    \n",
       "16 20            21             aufenthaltsdauer   29.38046    1     NA        \n",
       "17  0             0             NA                  0.00000   -1     Korb      \n",
       "18 22            23             altersunterschied  -1.50000    1     NA        \n",
       "19  0             0             NA                  0.00000   -1     Erfolg    \n",
       "20  0             0             NA                  0.00000   -1     Korb      \n",
       "21  0             0             NA                  0.00000   -1     Erfolg    \n",
       "22  0             0             NA                  0.00000   -1     Korb      \n",
       "23 24            25             haarfarbe           1.00000    1     NA        \n",
       "24  0             0             NA                  0.00000   -1     Erfolg    \n",
       "25  0             0             NA                  0.00000   -1     Korb      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "getTree(myRF, k=47, labelVar=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Beispiel Baum Nr.47 betrachten <br>\n",
    "labelvar : Name der Variablen zeigen <br>\n",
    "1.Zeile ist die Wurzel des Baumes. <br>\n",
    "Wenn Bedingung erfüllt ist(split var <= split point), dann \"left daughter\" anschauen. Falls nicht, \"right daughter\" anschauen. <br>\n",
    "Status = 1 : innerer Knoten , Status = -1 : Vorhersage (Korb, Erfolg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntree      OOB      1      2\n",
      "    1:  29.27% 47.62% 10.00%\n",
      "    2:  25.00% 46.43%  8.33%\n",
      "    3:  21.33% 36.36%  9.52%\n",
      "    4:  22.62% 33.33% 13.33%\n",
      "    5:  22.47% 35.00% 12.24%\n",
      "    6:  23.91% 32.50% 17.31%\n",
      "    7:  26.80% 35.00% 21.05%\n",
      "    8:  23.47% 30.00% 18.97%\n",
      "    9:  22.45% 30.00% 17.24%\n",
      "   10:  23.47% 32.50% 17.24%\n",
      "   11:  23.47% 37.50% 13.79%\n",
      "   12:  22.22% 32.50% 15.25%\n",
      "   13:  20.00% 30.00% 13.33%\n",
      "   14:  20.00% 27.50% 15.00%\n",
      "   15:  19.00% 27.50% 13.33%\n",
      "   16:  18.00% 27.50% 11.67%\n",
      "   17:  20.00% 30.00% 13.33%\n",
      "   18:  20.00% 30.00% 13.33%\n",
      "   19:  18.00% 30.00% 10.00%\n",
      "   20:  21.00% 30.00% 15.00%\n",
      "   21:  20.00% 30.00% 13.33%\n",
      "   22:  18.00% 30.00% 10.00%\n",
      "   23:  18.00% 30.00% 10.00%\n",
      "   24:  20.00% 32.50% 11.67%\n",
      "   25:  18.00% 30.00% 10.00%\n",
      "   26:  18.00% 30.00% 10.00%\n",
      "   27:  19.00% 32.50% 10.00%\n",
      "   28:  20.00% 27.50% 15.00%\n",
      "   29:  21.00% 32.50% 13.33%\n",
      "   30:  21.00% 32.50% 13.33%\n",
      "   31:  21.00% 30.00% 15.00%\n",
      "   32:  20.00% 27.50% 15.00%\n",
      "   33:  20.00% 30.00% 13.33%\n",
      "   34:  21.00% 30.00% 15.00%\n",
      "   35:  22.00% 32.50% 15.00%\n",
      "   36:  21.00% 30.00% 15.00%\n",
      "   37:  22.00% 32.50% 15.00%\n",
      "   38:  22.00% 32.50% 15.00%\n",
      "   39:  22.00% 32.50% 15.00%\n",
      "   40:  22.00% 32.50% 15.00%\n",
      "   41:  20.00% 30.00% 13.33%\n",
      "   42:  21.00% 30.00% 15.00%\n",
      "   43:  21.00% 32.50% 13.33%\n",
      "   44:  21.00% 32.50% 13.33%\n",
      "   45:  22.00% 32.50% 15.00%\n",
      "   46:  23.00% 35.00% 15.00%\n",
      "   47:  22.00% 32.50% 15.00%\n",
      "   48:  22.00% 32.50% 15.00%\n",
      "   49:  21.00% 32.50% 13.33%\n",
      "   50:  22.00% 35.00% 13.33%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       " randomForest(formula = ergebnis ~ ., data = datasetTrain, importance = TRUE,      mtry = 2, ntree = 100, replace = TRUE, do.trace = TRUE, keep.forest = TRUE) \n",
       "               Type of random forest: classification\n",
       "                     Number of trees: 150\n",
       "No. of variables tried at each split: 2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myRF2 <- grow(myRF, how.many=50)\n",
    "myRF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man kann auch über den `grow` Befehl einen Forest erweitern und einen neuen Forest erzeugen mit einer höheren Anzahl von Bäumen, falls die erste Anzahl der Bäume nicht ausreichend ist. Hier wird ein neuer Forest mit 150 Bäumen generiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       " randomForest(formula = ergebnis ~ ., data = datasetTrain, importance = TRUE,      mtry = 2, ntree = 100, replace = TRUE, do.trace = TRUE, keep.forest = TRUE) \n",
       "               Type of random forest: classification\n",
       "                     Number of trees: 250\n",
       "No. of variables tried at each split: 2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myRF3 <- combine(myRF, myRF2)\n",
    "myRF3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Außerdem kann man mit der `combine`- Methode mehrere Forests in einem Forest kombinieren. Hier werden die zwei vorher erstellten Modelle zusammengefasst und bilden ein neues Modell mit 250 Bäumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>36</li>\n",
       "\t<li>34</li>\n",
       "\t<li>34</li>\n",
       "\t<li>32</li>\n",
       "\t<li>46</li>\n",
       "\t<li>36</li>\n",
       "\t<li>44</li>\n",
       "\t<li>40</li>\n",
       "\t<li>39</li>\n",
       "\t<li>36</li>\n",
       "\t<li>33</li>\n",
       "\t<li>37</li>\n",
       "\t<li>43</li>\n",
       "\t<li>36</li>\n",
       "\t<li>36</li>\n",
       "\t<li>40</li>\n",
       "\t<li>33</li>\n",
       "\t<li>37</li>\n",
       "\t<li>30</li>\n",
       "\t<li>43</li>\n",
       "\t<li>31</li>\n",
       "\t<li>37</li>\n",
       "\t<li>31</li>\n",
       "\t<li>39</li>\n",
       "\t<li>32</li>\n",
       "\t<li>40</li>\n",
       "\t<li>33</li>\n",
       "\t<li>45</li>\n",
       "\t<li>36</li>\n",
       "\t<li>38</li>\n",
       "\t<li>38</li>\n",
       "\t<li>40</li>\n",
       "\t<li>37</li>\n",
       "\t<li>38</li>\n",
       "\t<li>40</li>\n",
       "\t<li>34</li>\n",
       "\t<li>29</li>\n",
       "\t<li>32</li>\n",
       "\t<li>32</li>\n",
       "\t<li>41</li>\n",
       "\t<li>31</li>\n",
       "\t<li>37</li>\n",
       "\t<li>39</li>\n",
       "\t<li>34</li>\n",
       "\t<li>36</li>\n",
       "\t<li>34</li>\n",
       "\t<li>36</li>\n",
       "\t<li>36</li>\n",
       "\t<li>38</li>\n",
       "\t<li>29</li>\n",
       "\t<li>33</li>\n",
       "\t<li>39</li>\n",
       "\t<li>43</li>\n",
       "\t<li>52</li>\n",
       "\t<li>42</li>\n",
       "\t<li>38</li>\n",
       "\t<li>28</li>\n",
       "\t<li>34</li>\n",
       "\t<li>38</li>\n",
       "\t<li>41</li>\n",
       "\t<li>39</li>\n",
       "\t<li>48</li>\n",
       "\t<li>30</li>\n",
       "\t<li>43</li>\n",
       "\t<li>35</li>\n",
       "\t<li>38</li>\n",
       "\t<li>43</li>\n",
       "\t<li>42</li>\n",
       "\t<li>31</li>\n",
       "\t<li>29</li>\n",
       "\t<li>37</li>\n",
       "\t<li>40</li>\n",
       "\t<li>37</li>\n",
       "\t<li>39</li>\n",
       "\t<li>47</li>\n",
       "\t<li>34</li>\n",
       "\t<li>34</li>\n",
       "\t<li>25</li>\n",
       "\t<li>42</li>\n",
       "\t<li>42</li>\n",
       "\t<li>42</li>\n",
       "\t<li>39</li>\n",
       "\t<li>37</li>\n",
       "\t<li>30</li>\n",
       "\t<li>38</li>\n",
       "\t<li>38</li>\n",
       "\t<li>42</li>\n",
       "\t<li>39</li>\n",
       "\t<li>43</li>\n",
       "\t<li>45</li>\n",
       "\t<li>42</li>\n",
       "\t<li>41</li>\n",
       "\t<li>42</li>\n",
       "\t<li>31</li>\n",
       "\t<li>38</li>\n",
       "\t<li>37</li>\n",
       "\t<li>27</li>\n",
       "\t<li>30</li>\n",
       "\t<li>46</li>\n",
       "\t<li>24</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 36\n",
       "\\item 34\n",
       "\\item 34\n",
       "\\item 32\n",
       "\\item 46\n",
       "\\item 36\n",
       "\\item 44\n",
       "\\item 40\n",
       "\\item 39\n",
       "\\item 36\n",
       "\\item 33\n",
       "\\item 37\n",
       "\\item 43\n",
       "\\item 36\n",
       "\\item 36\n",
       "\\item 40\n",
       "\\item 33\n",
       "\\item 37\n",
       "\\item 30\n",
       "\\item 43\n",
       "\\item 31\n",
       "\\item 37\n",
       "\\item 31\n",
       "\\item 39\n",
       "\\item 32\n",
       "\\item 40\n",
       "\\item 33\n",
       "\\item 45\n",
       "\\item 36\n",
       "\\item 38\n",
       "\\item 38\n",
       "\\item 40\n",
       "\\item 37\n",
       "\\item 38\n",
       "\\item 40\n",
       "\\item 34\n",
       "\\item 29\n",
       "\\item 32\n",
       "\\item 32\n",
       "\\item 41\n",
       "\\item 31\n",
       "\\item 37\n",
       "\\item 39\n",
       "\\item 34\n",
       "\\item 36\n",
       "\\item 34\n",
       "\\item 36\n",
       "\\item 36\n",
       "\\item 38\n",
       "\\item 29\n",
       "\\item 33\n",
       "\\item 39\n",
       "\\item 43\n",
       "\\item 52\n",
       "\\item 42\n",
       "\\item 38\n",
       "\\item 28\n",
       "\\item 34\n",
       "\\item 38\n",
       "\\item 41\n",
       "\\item 39\n",
       "\\item 48\n",
       "\\item 30\n",
       "\\item 43\n",
       "\\item 35\n",
       "\\item 38\n",
       "\\item 43\n",
       "\\item 42\n",
       "\\item 31\n",
       "\\item 29\n",
       "\\item 37\n",
       "\\item 40\n",
       "\\item 37\n",
       "\\item 39\n",
       "\\item 47\n",
       "\\item 34\n",
       "\\item 34\n",
       "\\item 25\n",
       "\\item 42\n",
       "\\item 42\n",
       "\\item 42\n",
       "\\item 39\n",
       "\\item 37\n",
       "\\item 30\n",
       "\\item 38\n",
       "\\item 38\n",
       "\\item 42\n",
       "\\item 39\n",
       "\\item 43\n",
       "\\item 45\n",
       "\\item 42\n",
       "\\item 41\n",
       "\\item 42\n",
       "\\item 31\n",
       "\\item 38\n",
       "\\item 37\n",
       "\\item 27\n",
       "\\item 30\n",
       "\\item 46\n",
       "\\item 24\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 36\n",
       "2. 34\n",
       "3. 34\n",
       "4. 32\n",
       "5. 46\n",
       "6. 36\n",
       "7. 44\n",
       "8. 40\n",
       "9. 39\n",
       "10. 36\n",
       "11. 33\n",
       "12. 37\n",
       "13. 43\n",
       "14. 36\n",
       "15. 36\n",
       "16. 40\n",
       "17. 33\n",
       "18. 37\n",
       "19. 30\n",
       "20. 43\n",
       "21. 31\n",
       "22. 37\n",
       "23. 31\n",
       "24. 39\n",
       "25. 32\n",
       "26. 40\n",
       "27. 33\n",
       "28. 45\n",
       "29. 36\n",
       "30. 38\n",
       "31. 38\n",
       "32. 40\n",
       "33. 37\n",
       "34. 38\n",
       "35. 40\n",
       "36. 34\n",
       "37. 29\n",
       "38. 32\n",
       "39. 32\n",
       "40. 41\n",
       "41. 31\n",
       "42. 37\n",
       "43. 39\n",
       "44. 34\n",
       "45. 36\n",
       "46. 34\n",
       "47. 36\n",
       "48. 36\n",
       "49. 38\n",
       "50. 29\n",
       "51. 33\n",
       "52. 39\n",
       "53. 43\n",
       "54. 52\n",
       "55. 42\n",
       "56. 38\n",
       "57. 28\n",
       "58. 34\n",
       "59. 38\n",
       "60. 41\n",
       "61. 39\n",
       "62. 48\n",
       "63. 30\n",
       "64. 43\n",
       "65. 35\n",
       "66. 38\n",
       "67. 43\n",
       "68. 42\n",
       "69. 31\n",
       "70. 29\n",
       "71. 37\n",
       "72. 40\n",
       "73. 37\n",
       "74. 39\n",
       "75. 47\n",
       "76. 34\n",
       "77. 34\n",
       "78. 25\n",
       "79. 42\n",
       "80. 42\n",
       "81. 42\n",
       "82. 39\n",
       "83. 37\n",
       "84. 30\n",
       "85. 38\n",
       "86. 38\n",
       "87. 42\n",
       "88. 39\n",
       "89. 43\n",
       "90. 45\n",
       "91. 42\n",
       "92. 41\n",
       "93. 42\n",
       "94. 31\n",
       "95. 38\n",
       "96. 37\n",
       "97. 27\n",
       "98. 30\n",
       "99. 46\n",
       "100. 24\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1] 36 34 34 32 46 36 44 40 39 36 33 37 43 36 36 40 33 37 30 43 31 37 31 39 32\n",
       " [26] 40 33 45 36 38 38 40 37 38 40 34 29 32 32 41 31 37 39 34 36 34 36 36 38 29\n",
       " [51] 33 39 43 52 42 38 28 34 38 41 39 48 30 43 35 38 43 42 31 29 37 40 37 39 47\n",
       " [76] 34 34 25 42 42 42 39 37 30 38 38 42 39 43 45 42 41 42 31 38 37 27 30 46 24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "37.12"
      ],
      "text/latex": [
       "37.12"
      ],
      "text/markdown": [
       "37.12"
      ],
      "text/plain": [
       "[1] 37.12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myRF$oob.times\n",
    "mean(myRF$oob.times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OOB = Out Of the Bag Fehlerschätzer <br>\n",
    "Der OOB-Fehlerschätzer:\n",
    "- sucht alle Bäume heraus,bei denen der erste Mann nicht für die Erstellung dieser Bäume ausgewählt wurde. <br>\n",
    "- erstellt einen neuen Forest mit diesen Bäumen und sendet diesen Mann durch den neuen Forest.  <br>\n",
    "- gibt eine Antwort zu der Frage : \"Wie viel Prozent der 100 Vorhersagen richtig waren und wie viele falsch?\" und <br>\n",
    "- gibt eine gute Darstellung des Generalisierungsfehler an. <br>\n",
    "\n",
    "Fazit: Der erster Mann wurde 36 Mal nicht für die Erstellung eines Baumes ausgewählt usw. und jeder Mann wurde im Durchschnitt ca.37 Mal nicht für die Erstellung eines Baumes ausgewählt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        \n",
       "         Erfolg Korb\n",
       "  Erfolg     31    9\n",
       "  Korb        9   51"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(datasetTrain$ergebnis, myRF$predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kontigenztabele/ Konfusionsmatrix mit den OOB-Daten <br>\n",
    "Damit kann man die Frage : \"Mit wie vielen OOB Schätzer habe ich mich vertan?\" beantworten.<br>\n",
    "- 31 Männer hatten Erfolg und wurde auch Erfolg vorhergesagt.<br>\n",
    "- 9 Männer hatten Erfolg, aber wurde Korb vorhergesagt. <br>\n",
    "usw.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>OOB</th><th scope=col>Erfolg</th><th scope=col>Korb</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.1621622</td><td>0.1538462</td><td>0.1666667</td></tr>\n",
       "\t<tr><td>0.1612903</td><td>0.1500000</td><td>0.1666667</td></tr>\n",
       "\t<tr><td>0.1710526</td><td>0.1200000</td><td>0.1960784</td></tr>\n",
       "\t<tr><td>0.1839080</td><td>0.1562500</td><td>0.2000000</td></tr>\n",
       "\t<tr><td>0.1868132</td><td>0.1212121</td><td>0.2241379</td></tr>\n",
       "\t<tr><td>0.2417582</td><td>0.3030303</td><td>0.2068966</td></tr>\n",
       "\t<tr><td>0.2105263</td><td>0.1666667</td><td>0.2372881</td></tr>\n",
       "\t<tr><td>0.2040816</td><td>0.1538462</td><td>0.2372881</td></tr>\n",
       "\t<tr><td>0.1818182</td><td>0.2250000</td><td>0.1525424</td></tr>\n",
       "\t<tr><td>0.1919192</td><td>0.2250000</td><td>0.1694915</td></tr>\n",
       "\t<tr><td>0.1919192</td><td>0.2250000</td><td>0.1694915</td></tr>\n",
       "\t<tr><td>0.1818182</td><td>0.2000000</td><td>0.1694915</td></tr>\n",
       "\t<tr><td>0.1818182</td><td>0.2250000</td><td>0.1525424</td></tr>\n",
       "\t<tr><td>0.1818182</td><td>0.2250000</td><td>0.1525424</td></tr>\n",
       "\t<tr><td>0.1800000</td><td>0.2250000</td><td>0.1500000</td></tr>\n",
       "\t<tr><td>0.1700000</td><td>0.2000000</td><td>0.1500000</td></tr>\n",
       "\t<tr><td>0.1700000</td><td>0.2000000</td><td>0.1500000</td></tr>\n",
       "\t<tr><td>0.1700000</td><td>0.1750000</td><td>0.1666667</td></tr>\n",
       "\t<tr><td>0.1600000</td><td>0.1750000</td><td>0.1500000</td></tr>\n",
       "\t<tr><td>0.1700000</td><td>0.2000000</td><td>0.1500000</td></tr>\n",
       "\t<tr><td>0.1700000</td><td>0.1750000</td><td>0.1666667</td></tr>\n",
       "\t<tr><td>0.1800000</td><td>0.2000000</td><td>0.1666667</td></tr>\n",
       "\t<tr><td>0.1800000</td><td>0.1750000</td><td>0.1833333</td></tr>\n",
       "\t<tr><td>0.1800000</td><td>0.1750000</td><td>0.1833333</td></tr>\n",
       "\t<tr><td>0.1900000</td><td>0.2000000</td><td>0.1833333</td></tr>\n",
       "\t<tr><td>0.2000000</td><td>0.2250000</td><td>0.1833333</td></tr>\n",
       "\t<tr><td>0.1600000</td><td>0.1750000</td><td>0.1500000</td></tr>\n",
       "\t<tr><td>0.1800000</td><td>0.1750000</td><td>0.1833333</td></tr>\n",
       "\t<tr><td>0.1800000</td><td>0.2000000</td><td>0.1666667</td></tr>\n",
       "\t<tr><td>0.1900000</td><td>0.2250000</td><td>0.1666667</td></tr>\n",
       "\t<tr><td>...</td><td>...</td><td>...</td></tr>\n",
       "\t<tr><td>0.17 </td><td>0.200</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.17 </td><td>0.200</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.18 </td><td>0.225</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.19 </td><td>0.250</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.17 </td><td>0.200</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.18 </td><td>0.225</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.19 </td><td>0.250</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.20 </td><td>0.275</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.19 </td><td>0.250</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.19 </td><td>0.250</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.19 </td><td>0.250</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.19 </td><td>0.250</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.19 </td><td>0.250</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.19 </td><td>0.250</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.18 </td><td>0.225</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.18 </td><td>0.225</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.18 </td><td>0.225</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.19 </td><td>0.250</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.19 </td><td>0.250</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.18 </td><td>0.225</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.19 </td><td>0.250</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.18 </td><td>0.225</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.18 </td><td>0.225</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.17 </td><td>0.200</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.18 </td><td>0.225</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.18 </td><td>0.225</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.18 </td><td>0.225</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.18 </td><td>0.225</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.18 </td><td>0.225</td><td>0.15 </td></tr>\n",
       "\t<tr><td>0.18 </td><td>0.225</td><td>0.15 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{lll}\n",
       " OOB & Erfolg & Korb\\\\\n",
       "\\hline\n",
       "\t 0.1621622 & 0.1538462 & 0.1666667\\\\\n",
       "\t 0.1612903 & 0.1500000 & 0.1666667\\\\\n",
       "\t 0.1710526 & 0.1200000 & 0.1960784\\\\\n",
       "\t 0.1839080 & 0.1562500 & 0.2000000\\\\\n",
       "\t 0.1868132 & 0.1212121 & 0.2241379\\\\\n",
       "\t 0.2417582 & 0.3030303 & 0.2068966\\\\\n",
       "\t 0.2105263 & 0.1666667 & 0.2372881\\\\\n",
       "\t 0.2040816 & 0.1538462 & 0.2372881\\\\\n",
       "\t 0.1818182 & 0.2250000 & 0.1525424\\\\\n",
       "\t 0.1919192 & 0.2250000 & 0.1694915\\\\\n",
       "\t 0.1919192 & 0.2250000 & 0.1694915\\\\\n",
       "\t 0.1818182 & 0.2000000 & 0.1694915\\\\\n",
       "\t 0.1818182 & 0.2250000 & 0.1525424\\\\\n",
       "\t 0.1818182 & 0.2250000 & 0.1525424\\\\\n",
       "\t 0.1800000 & 0.2250000 & 0.1500000\\\\\n",
       "\t 0.1700000 & 0.2000000 & 0.1500000\\\\\n",
       "\t 0.1700000 & 0.2000000 & 0.1500000\\\\\n",
       "\t 0.1700000 & 0.1750000 & 0.1666667\\\\\n",
       "\t 0.1600000 & 0.1750000 & 0.1500000\\\\\n",
       "\t 0.1700000 & 0.2000000 & 0.1500000\\\\\n",
       "\t 0.1700000 & 0.1750000 & 0.1666667\\\\\n",
       "\t 0.1800000 & 0.2000000 & 0.1666667\\\\\n",
       "\t 0.1800000 & 0.1750000 & 0.1833333\\\\\n",
       "\t 0.1800000 & 0.1750000 & 0.1833333\\\\\n",
       "\t 0.1900000 & 0.2000000 & 0.1833333\\\\\n",
       "\t 0.2000000 & 0.2250000 & 0.1833333\\\\\n",
       "\t 0.1600000 & 0.1750000 & 0.1500000\\\\\n",
       "\t 0.1800000 & 0.1750000 & 0.1833333\\\\\n",
       "\t 0.1800000 & 0.2000000 & 0.1666667\\\\\n",
       "\t 0.1900000 & 0.2250000 & 0.1666667\\\\\n",
       "\t ... & ... & ...\\\\\n",
       "\t 0.17  & 0.200 & 0.15 \\\\\n",
       "\t 0.17  & 0.200 & 0.15 \\\\\n",
       "\t 0.18  & 0.225 & 0.15 \\\\\n",
       "\t 0.19  & 0.250 & 0.15 \\\\\n",
       "\t 0.17  & 0.200 & 0.15 \\\\\n",
       "\t 0.18  & 0.225 & 0.15 \\\\\n",
       "\t 0.19  & 0.250 & 0.15 \\\\\n",
       "\t 0.20  & 0.275 & 0.15 \\\\\n",
       "\t 0.19  & 0.250 & 0.15 \\\\\n",
       "\t 0.19  & 0.250 & 0.15 \\\\\n",
       "\t 0.19  & 0.250 & 0.15 \\\\\n",
       "\t 0.19  & 0.250 & 0.15 \\\\\n",
       "\t 0.19  & 0.250 & 0.15 \\\\\n",
       "\t 0.19  & 0.250 & 0.15 \\\\\n",
       "\t 0.18  & 0.225 & 0.15 \\\\\n",
       "\t 0.18  & 0.225 & 0.15 \\\\\n",
       "\t 0.18  & 0.225 & 0.15 \\\\\n",
       "\t 0.19  & 0.250 & 0.15 \\\\\n",
       "\t 0.19  & 0.250 & 0.15 \\\\\n",
       "\t 0.18  & 0.225 & 0.15 \\\\\n",
       "\t 0.19  & 0.250 & 0.15 \\\\\n",
       "\t 0.18  & 0.225 & 0.15 \\\\\n",
       "\t 0.18  & 0.225 & 0.15 \\\\\n",
       "\t 0.17  & 0.200 & 0.15 \\\\\n",
       "\t 0.18  & 0.225 & 0.15 \\\\\n",
       "\t 0.18  & 0.225 & 0.15 \\\\\n",
       "\t 0.18  & 0.225 & 0.15 \\\\\n",
       "\t 0.18  & 0.225 & 0.15 \\\\\n",
       "\t 0.18  & 0.225 & 0.15 \\\\\n",
       "\t 0.18  & 0.225 & 0.15 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "OOB | Erfolg | Korb | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 0.1621622 | 0.1538462 | 0.1666667 | \n",
       "| 0.1612903 | 0.1500000 | 0.1666667 | \n",
       "| 0.1710526 | 0.1200000 | 0.1960784 | \n",
       "| 0.1839080 | 0.1562500 | 0.2000000 | \n",
       "| 0.1868132 | 0.1212121 | 0.2241379 | \n",
       "| 0.2417582 | 0.3030303 | 0.2068966 | \n",
       "| 0.2105263 | 0.1666667 | 0.2372881 | \n",
       "| 0.2040816 | 0.1538462 | 0.2372881 | \n",
       "| 0.1818182 | 0.2250000 | 0.1525424 | \n",
       "| 0.1919192 | 0.2250000 | 0.1694915 | \n",
       "| 0.1919192 | 0.2250000 | 0.1694915 | \n",
       "| 0.1818182 | 0.2000000 | 0.1694915 | \n",
       "| 0.1818182 | 0.2250000 | 0.1525424 | \n",
       "| 0.1818182 | 0.2250000 | 0.1525424 | \n",
       "| 0.1800000 | 0.2250000 | 0.1500000 | \n",
       "| 0.1700000 | 0.2000000 | 0.1500000 | \n",
       "| 0.1700000 | 0.2000000 | 0.1500000 | \n",
       "| 0.1700000 | 0.1750000 | 0.1666667 | \n",
       "| 0.1600000 | 0.1750000 | 0.1500000 | \n",
       "| 0.1700000 | 0.2000000 | 0.1500000 | \n",
       "| 0.1700000 | 0.1750000 | 0.1666667 | \n",
       "| 0.1800000 | 0.2000000 | 0.1666667 | \n",
       "| 0.1800000 | 0.1750000 | 0.1833333 | \n",
       "| 0.1800000 | 0.1750000 | 0.1833333 | \n",
       "| 0.1900000 | 0.2000000 | 0.1833333 | \n",
       "| 0.2000000 | 0.2250000 | 0.1833333 | \n",
       "| 0.1600000 | 0.1750000 | 0.1500000 | \n",
       "| 0.1800000 | 0.1750000 | 0.1833333 | \n",
       "| 0.1800000 | 0.2000000 | 0.1666667 | \n",
       "| 0.1900000 | 0.2250000 | 0.1666667 | \n",
       "| ... | ... | ... | \n",
       "| 0.17  | 0.200 | 0.15  | \n",
       "| 0.17  | 0.200 | 0.15  | \n",
       "| 0.18  | 0.225 | 0.15  | \n",
       "| 0.19  | 0.250 | 0.15  | \n",
       "| 0.17  | 0.200 | 0.15  | \n",
       "| 0.18  | 0.225 | 0.15  | \n",
       "| 0.19  | 0.250 | 0.15  | \n",
       "| 0.20  | 0.275 | 0.15  | \n",
       "| 0.19  | 0.250 | 0.15  | \n",
       "| 0.19  | 0.250 | 0.15  | \n",
       "| 0.19  | 0.250 | 0.15  | \n",
       "| 0.19  | 0.250 | 0.15  | \n",
       "| 0.19  | 0.250 | 0.15  | \n",
       "| 0.19  | 0.250 | 0.15  | \n",
       "| 0.18  | 0.225 | 0.15  | \n",
       "| 0.18  | 0.225 | 0.15  | \n",
       "| 0.18  | 0.225 | 0.15  | \n",
       "| 0.19  | 0.250 | 0.15  | \n",
       "| 0.19  | 0.250 | 0.15  | \n",
       "| 0.18  | 0.225 | 0.15  | \n",
       "| 0.19  | 0.250 | 0.15  | \n",
       "| 0.18  | 0.225 | 0.15  | \n",
       "| 0.18  | 0.225 | 0.15  | \n",
       "| 0.17  | 0.200 | 0.15  | \n",
       "| 0.18  | 0.225 | 0.15  | \n",
       "| 0.18  | 0.225 | 0.15  | \n",
       "| 0.18  | 0.225 | 0.15  | \n",
       "| 0.18  | 0.225 | 0.15  | \n",
       "| 0.18  | 0.225 | 0.15  | \n",
       "| 0.18  | 0.225 | 0.15  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "      OOB       Erfolg    Korb     \n",
       " [1,] 0.1621622 0.1538462 0.1666667\n",
       " [2,] 0.1612903 0.1500000 0.1666667\n",
       " [3,] 0.1710526 0.1200000 0.1960784\n",
       " [4,] 0.1839080 0.1562500 0.2000000\n",
       " [5,] 0.1868132 0.1212121 0.2241379\n",
       " [6,] 0.2417582 0.3030303 0.2068966\n",
       " [7,] 0.2105263 0.1666667 0.2372881\n",
       " [8,] 0.2040816 0.1538462 0.2372881\n",
       " [9,] 0.1818182 0.2250000 0.1525424\n",
       "[10,] 0.1919192 0.2250000 0.1694915\n",
       "[11,] 0.1919192 0.2250000 0.1694915\n",
       "[12,] 0.1818182 0.2000000 0.1694915\n",
       "[13,] 0.1818182 0.2250000 0.1525424\n",
       "[14,] 0.1818182 0.2250000 0.1525424\n",
       "[15,] 0.1800000 0.2250000 0.1500000\n",
       "[16,] 0.1700000 0.2000000 0.1500000\n",
       "[17,] 0.1700000 0.2000000 0.1500000\n",
       "[18,] 0.1700000 0.1750000 0.1666667\n",
       "[19,] 0.1600000 0.1750000 0.1500000\n",
       "[20,] 0.1700000 0.2000000 0.1500000\n",
       "[21,] 0.1700000 0.1750000 0.1666667\n",
       "[22,] 0.1800000 0.2000000 0.1666667\n",
       "[23,] 0.1800000 0.1750000 0.1833333\n",
       "[24,] 0.1800000 0.1750000 0.1833333\n",
       "[25,] 0.1900000 0.2000000 0.1833333\n",
       "[26,] 0.2000000 0.2250000 0.1833333\n",
       "[27,] 0.1600000 0.1750000 0.1500000\n",
       "[28,] 0.1800000 0.1750000 0.1833333\n",
       "[29,] 0.1800000 0.2000000 0.1666667\n",
       "[30,] 0.1900000 0.2250000 0.1666667\n",
       "[31,] ...       ...       ...      \n",
       "[32,] 0.17      0.200     0.15     \n",
       "[33,] 0.17      0.200     0.15     \n",
       "[34,] 0.18      0.225     0.15     \n",
       "[35,] 0.19      0.250     0.15     \n",
       "[36,] 0.17      0.200     0.15     \n",
       "[37,] 0.18      0.225     0.15     \n",
       "[38,] 0.19      0.250     0.15     \n",
       "[39,] 0.20      0.275     0.15     \n",
       "[40,] 0.19      0.250     0.15     \n",
       "[41,] 0.19      0.250     0.15     \n",
       "[42,] 0.19      0.250     0.15     \n",
       "[43,] 0.19      0.250     0.15     \n",
       "[44,] 0.19      0.250     0.15     \n",
       "[45,] 0.19      0.250     0.15     \n",
       "[46,] 0.18      0.225     0.15     \n",
       "[47,] 0.18      0.225     0.15     \n",
       "[48,] 0.18      0.225     0.15     \n",
       "[49,] 0.19      0.250     0.15     \n",
       "[50,] 0.19      0.250     0.15     \n",
       "[51,] 0.18      0.225     0.15     \n",
       "[52,] 0.19      0.250     0.15     \n",
       "[53,] 0.18      0.225     0.15     \n",
       "[54,] 0.18      0.225     0.15     \n",
       "[55,] 0.17      0.200     0.15     \n",
       "[56,] 0.18      0.225     0.15     \n",
       "[57,] 0.18      0.225     0.15     \n",
       "[58,] 0.18      0.225     0.15     \n",
       "[59,] 0.18      0.225     0.15     \n",
       "[60,] 0.18      0.225     0.15     \n",
       "[61,] 0.18      0.225     0.15     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myRF$err.rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OOB-Fehlerschätzung. Hier wird jeder Baum seperat betrachtet und sein OOB-Fehler ausgegeben. Der letzte Wert des OOB-Fehlers ist der gesamte OOB-Fehler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(myRF$err.rate[,1], type=\"l\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei dieser graphischen Darstellung kann man erkennen, welche Anzahl der Bäume, eine gute Anzahl für das erzeugte Random-Forest-Modell sind. Bei einer Anzahl von 20 Bäumen schwankt der OBB-Fehler sehr stark, dass heißt 20 Bäume sind keine gute Anzahl für das Modell. 100 Bäume sind eine sehr plausible Anzahl von Bäumen, da der OOB-Fehler stabiler ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Korb</li>\n",
       "\t<li>Erfolg</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Erfolg\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Erfolg\n",
       "\\item Erfolg\n",
       "\\item Erfolg\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Erfolg\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Erfolg\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\item Korb\n",
       "\\item Korb\n",
       "\\item Erfolg\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. Korb\n",
       "2. Korb\n",
       "3. Erfolg\n",
       "4. Erfolg\n",
       "5. Korb\n",
       "6. Erfolg\n",
       "7. Korb\n",
       "8. Korb\n",
       "9. Korb\n",
       "10. Korb\n",
       "11. Erfolg\n",
       "12. Korb\n",
       "13. Erfolg\n",
       "14. Erfolg\n",
       "15. Korb\n",
       "16. Korb\n",
       "17. Korb\n",
       "18. Erfolg\n",
       "19. Korb\n",
       "20. Erfolg\n",
       "21. Erfolg\n",
       "22. Korb\n",
       "23. Korb\n",
       "24. Korb\n",
       "25. Erfolg\n",
       "26. Erfolg\n",
       "27. Korb\n",
       "28. Korb\n",
       "29. Korb\n",
       "30. Korb\n",
       "31. Korb\n",
       "32. Erfolg\n",
       "33. Korb\n",
       "34. Erfolg\n",
       "35. Korb\n",
       "36. Erfolg\n",
       "37. Erfolg\n",
       "38. Korb\n",
       "39. Erfolg\n",
       "40. Korb\n",
       "41. Korb\n",
       "42. Erfolg\n",
       "43. Erfolg\n",
       "44. Korb\n",
       "45. Erfolg\n",
       "46. Erfolg\n",
       "47. Erfolg\n",
       "48. Korb\n",
       "49. Korb\n",
       "50. Korb\n",
       "51. Korb\n",
       "52. Korb\n",
       "53. Erfolg\n",
       "54. Erfolg\n",
       "55. Erfolg\n",
       "56. Erfolg\n",
       "57. Erfolg\n",
       "58. Korb\n",
       "59. Erfolg\n",
       "60. Korb\n",
       "61. Korb\n",
       "62. Korb\n",
       "63. Erfolg\n",
       "64. Erfolg\n",
       "65. Erfolg\n",
       "66. Korb\n",
       "67. Korb\n",
       "68. Erfolg\n",
       "69. Erfolg\n",
       "70. Erfolg\n",
       "71. Korb\n",
       "72. Erfolg\n",
       "73. Korb\n",
       "74. Korb\n",
       "75. Korb\n",
       "76. Korb\n",
       "77. Korb\n",
       "78. Korb\n",
       "79. Korb\n",
       "80. Korb\n",
       "81. Korb\n",
       "82. Erfolg\n",
       "83. Korb\n",
       "84. Erfolg\n",
       "85. Erfolg\n",
       "86. Korb\n",
       "87. Erfolg\n",
       "88. Korb\n",
       "89. Korb\n",
       "90. Korb\n",
       "91. Korb\n",
       "92. Korb\n",
       "93. Korb\n",
       "94. Korb\n",
       "95. Korb\n",
       "96. Korb\n",
       "97. Erfolg\n",
       "98. Korb\n",
       "99. Korb\n",
       "100. Erfolg\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1] Korb   Korb   Erfolg Erfolg Korb   Erfolg Korb   Korb   Korb   Korb  \n",
       " [11] Erfolg Korb   Erfolg Erfolg Korb   Korb   Korb   Erfolg Korb   Erfolg\n",
       " [21] Erfolg Korb   Korb   Korb   Erfolg Erfolg Korb   Korb   Korb   Korb  \n",
       " [31] Korb   Erfolg Korb   Erfolg Korb   Erfolg Erfolg Korb   Erfolg Korb  \n",
       " [41] Korb   Erfolg Erfolg Korb   Erfolg Erfolg Erfolg Korb   Korb   Korb  \n",
       " [51] Korb   Korb   Erfolg Erfolg Erfolg Erfolg Erfolg Korb   Erfolg Korb  \n",
       " [61] Korb   Korb   Erfolg Erfolg Erfolg Korb   Korb   Erfolg Erfolg Erfolg\n",
       " [71] Korb   Erfolg Korb   Korb   Korb   Korb   Korb   Korb   Korb   Korb  \n",
       " [81] Korb   Erfolg Korb   Erfolg Erfolg Korb   Erfolg Korb   Korb   Korb  \n",
       " [91] Korb   Korb   Korb   Korb   Korb   Korb   Erfolg Korb   Korb   Erfolg\n",
       "Levels: Erfolg Korb"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wahreKlasseTrain <- datasetTrain$ergebnis\n",
    "vorhersage1 <- predict(myRF, newdata=datasetTrain)\n",
    "wahreKlasseTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>1</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>2</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>3</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>4</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>5</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>6</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>7</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>8</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>9</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>10</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>11</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>12</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>13</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>14</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>15</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>16</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>17</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>18</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>19</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>20</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>21</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>22</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>23</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>24</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>25</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>26</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>27</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>28</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>29</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>30</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>31</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>32</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>33</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>34</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>35</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>36</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>37</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>38</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>39</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>40</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>41</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>42</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>43</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>44</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>45</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>46</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>47</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>48</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>49</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>50</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>51</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>52</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>53</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>54</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>55</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>56</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>57</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>58</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>59</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>60</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>61</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>62</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>63</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>64</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>65</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>66</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>67</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>68</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>69</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>70</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>71</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>72</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>73</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>74</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>75</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>76</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>77</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>78</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>79</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>80</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>81</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>82</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>83</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>84</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>85</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>86</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>87</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>88</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>89</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>90</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>91</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>92</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>93</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>94</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>95</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>96</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>97</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "\t<dt>98</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>99</dt>\n",
       "\t\t<dd>Korb</dd>\n",
       "\t<dt>100</dt>\n",
       "\t\t<dd>Erfolg</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[1] Korb\n",
       "\\item[2] Korb\n",
       "\\item[3] Erfolg\n",
       "\\item[4] Erfolg\n",
       "\\item[5] Korb\n",
       "\\item[6] Erfolg\n",
       "\\item[7] Korb\n",
       "\\item[8] Korb\n",
       "\\item[9] Korb\n",
       "\\item[10] Korb\n",
       "\\item[11] Erfolg\n",
       "\\item[12] Korb\n",
       "\\item[13] Erfolg\n",
       "\\item[14] Erfolg\n",
       "\\item[15] Korb\n",
       "\\item[16] Korb\n",
       "\\item[17] Korb\n",
       "\\item[18] Erfolg\n",
       "\\item[19] Korb\n",
       "\\item[20] Erfolg\n",
       "\\item[21] Erfolg\n",
       "\\item[22] Korb\n",
       "\\item[23] Korb\n",
       "\\item[24] Korb\n",
       "\\item[25] Erfolg\n",
       "\\item[26] Erfolg\n",
       "\\item[27] Korb\n",
       "\\item[28] Korb\n",
       "\\item[29] Korb\n",
       "\\item[30] Korb\n",
       "\\item[31] Korb\n",
       "\\item[32] Erfolg\n",
       "\\item[33] Korb\n",
       "\\item[34] Erfolg\n",
       "\\item[35] Korb\n",
       "\\item[36] Erfolg\n",
       "\\item[37] Erfolg\n",
       "\\item[38] Korb\n",
       "\\item[39] Erfolg\n",
       "\\item[40] Korb\n",
       "\\item[41] Korb\n",
       "\\item[42] Erfolg\n",
       "\\item[43] Erfolg\n",
       "\\item[44] Korb\n",
       "\\item[45] Erfolg\n",
       "\\item[46] Erfolg\n",
       "\\item[47] Erfolg\n",
       "\\item[48] Korb\n",
       "\\item[49] Korb\n",
       "\\item[50] Korb\n",
       "\\item[51] Korb\n",
       "\\item[52] Korb\n",
       "\\item[53] Erfolg\n",
       "\\item[54] Erfolg\n",
       "\\item[55] Erfolg\n",
       "\\item[56] Erfolg\n",
       "\\item[57] Erfolg\n",
       "\\item[58] Korb\n",
       "\\item[59] Erfolg\n",
       "\\item[60] Korb\n",
       "\\item[61] Korb\n",
       "\\item[62] Korb\n",
       "\\item[63] Erfolg\n",
       "\\item[64] Erfolg\n",
       "\\item[65] Erfolg\n",
       "\\item[66] Korb\n",
       "\\item[67] Korb\n",
       "\\item[68] Erfolg\n",
       "\\item[69] Erfolg\n",
       "\\item[70] Erfolg\n",
       "\\item[71] Korb\n",
       "\\item[72] Erfolg\n",
       "\\item[73] Korb\n",
       "\\item[74] Korb\n",
       "\\item[75] Korb\n",
       "\\item[76] Korb\n",
       "\\item[77] Korb\n",
       "\\item[78] Korb\n",
       "\\item[79] Korb\n",
       "\\item[80] Korb\n",
       "\\item[81] Korb\n",
       "\\item[82] Erfolg\n",
       "\\item[83] Korb\n",
       "\\item[84] Erfolg\n",
       "\\item[85] Erfolg\n",
       "\\item[86] Korb\n",
       "\\item[87] Erfolg\n",
       "\\item[88] Korb\n",
       "\\item[89] Korb\n",
       "\\item[90] Korb\n",
       "\\item[91] Korb\n",
       "\\item[92] Korb\n",
       "\\item[93] Korb\n",
       "\\item[94] Korb\n",
       "\\item[95] Korb\n",
       "\\item[96] Korb\n",
       "\\item[97] Erfolg\n",
       "\\item[98] Korb\n",
       "\\item[99] Korb\n",
       "\\item[100] Erfolg\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "1\n",
       ":   Korb2\n",
       ":   Korb3\n",
       ":   Erfolg4\n",
       ":   Erfolg5\n",
       ":   Korb6\n",
       ":   Erfolg7\n",
       ":   Korb8\n",
       ":   Korb9\n",
       ":   Korb10\n",
       ":   Korb11\n",
       ":   Erfolg12\n",
       ":   Korb13\n",
       ":   Erfolg14\n",
       ":   Erfolg15\n",
       ":   Korb16\n",
       ":   Korb17\n",
       ":   Korb18\n",
       ":   Erfolg19\n",
       ":   Korb20\n",
       ":   Erfolg21\n",
       ":   Erfolg22\n",
       ":   Korb23\n",
       ":   Korb24\n",
       ":   Korb25\n",
       ":   Erfolg26\n",
       ":   Erfolg27\n",
       ":   Korb28\n",
       ":   Korb29\n",
       ":   Korb30\n",
       ":   Korb31\n",
       ":   Korb32\n",
       ":   Erfolg33\n",
       ":   Korb34\n",
       ":   Erfolg35\n",
       ":   Korb36\n",
       ":   Erfolg37\n",
       ":   Erfolg38\n",
       ":   Korb39\n",
       ":   Erfolg40\n",
       ":   Korb41\n",
       ":   Korb42\n",
       ":   Erfolg43\n",
       ":   Erfolg44\n",
       ":   Korb45\n",
       ":   Erfolg46\n",
       ":   Erfolg47\n",
       ":   Erfolg48\n",
       ":   Korb49\n",
       ":   Korb50\n",
       ":   Korb51\n",
       ":   Korb52\n",
       ":   Korb53\n",
       ":   Erfolg54\n",
       ":   Erfolg55\n",
       ":   Erfolg56\n",
       ":   Erfolg57\n",
       ":   Erfolg58\n",
       ":   Korb59\n",
       ":   Erfolg60\n",
       ":   Korb61\n",
       ":   Korb62\n",
       ":   Korb63\n",
       ":   Erfolg64\n",
       ":   Erfolg65\n",
       ":   Erfolg66\n",
       ":   Korb67\n",
       ":   Korb68\n",
       ":   Erfolg69\n",
       ":   Erfolg70\n",
       ":   Erfolg71\n",
       ":   Korb72\n",
       ":   Erfolg73\n",
       ":   Korb74\n",
       ":   Korb75\n",
       ":   Korb76\n",
       ":   Korb77\n",
       ":   Korb78\n",
       ":   Korb79\n",
       ":   Korb80\n",
       ":   Korb81\n",
       ":   Korb82\n",
       ":   Erfolg83\n",
       ":   Korb84\n",
       ":   Erfolg85\n",
       ":   Erfolg86\n",
       ":   Korb87\n",
       ":   Erfolg88\n",
       ":   Korb89\n",
       ":   Korb90\n",
       ":   Korb91\n",
       ":   Korb92\n",
       ":   Korb93\n",
       ":   Korb94\n",
       ":   Korb95\n",
       ":   Korb96\n",
       ":   Korb97\n",
       ":   Erfolg98\n",
       ":   Korb99\n",
       ":   Korb100\n",
       ":   Erfolg\n",
       "\n"
      ],
      "text/plain": [
       "     1      2      3      4      5      6      7      8      9     10     11 \n",
       "  Korb   Korb Erfolg Erfolg   Korb Erfolg   Korb   Korb   Korb   Korb Erfolg \n",
       "    12     13     14     15     16     17     18     19     20     21     22 \n",
       "  Korb Erfolg Erfolg   Korb   Korb   Korb Erfolg   Korb Erfolg Erfolg   Korb \n",
       "    23     24     25     26     27     28     29     30     31     32     33 \n",
       "  Korb   Korb Erfolg Erfolg   Korb   Korb   Korb   Korb   Korb Erfolg   Korb \n",
       "    34     35     36     37     38     39     40     41     42     43     44 \n",
       "Erfolg   Korb Erfolg Erfolg   Korb Erfolg   Korb   Korb Erfolg Erfolg   Korb \n",
       "    45     46     47     48     49     50     51     52     53     54     55 \n",
       "Erfolg Erfolg Erfolg   Korb   Korb   Korb   Korb   Korb Erfolg Erfolg Erfolg \n",
       "    56     57     58     59     60     61     62     63     64     65     66 \n",
       "Erfolg Erfolg   Korb Erfolg   Korb   Korb   Korb Erfolg Erfolg Erfolg   Korb \n",
       "    67     68     69     70     71     72     73     74     75     76     77 \n",
       "  Korb Erfolg Erfolg Erfolg   Korb Erfolg   Korb   Korb   Korb   Korb   Korb \n",
       "    78     79     80     81     82     83     84     85     86     87     88 \n",
       "  Korb   Korb   Korb   Korb Erfolg   Korb Erfolg Erfolg   Korb Erfolg   Korb \n",
       "    89     90     91     92     93     94     95     96     97     98     99 \n",
       "  Korb   Korb   Korb   Korb   Korb   Korb   Korb   Korb Erfolg   Korb   Korb \n",
       "   100 \n",
       "Erfolg \n",
       "Levels: Erfolg Korb"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vorhersage1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier kann man die richtigen Ergebnisse und die Vorhersagen für jeden Mann betrachten. Bei der Variablen `wahreKlasseTrain` stehen die wahren Ergebnisse der Männern. In der Variable `Vorhersage1` stehen die Vorhersagen des Forests. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                vorhersage1\n",
       "wahleKlasseTrain Erfolg Korb\n",
       "          Erfolg     40    0\n",
       "          Korb        0   60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(wahreKlasseTrain,vorhersage1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Kontigenztabelle zeigt uns, welche Männer richtig klassifiziert  sind und welche falsch. Unser Forest ist elfolgreich, da alle Männer richtig klassifiziert wurden. Wir haben den Datensatz genommen, mit dem wir das Random Forest generiert haben und darum kann man jetzt nicht behaupten, dass es der Realität entspricht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               vorhersage2\n",
       "wahleKlasseTest Erfolg Korb\n",
       "         Erfolg     36    4\n",
       "         Korb        9   51"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Berechnung des Generalisierungsfehlers\n",
    "wahreKlasseTest <- datasetTest$ergebnis\n",
    "vorhersage2 <- predict(myRF, newdata=datasetTest)\n",
    "\n",
    "tab <- table(wahreKlasseTest,vorhersage2)\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für diese Kontigenztabelle habe ich den Datensatz `datasetTest` benutzt. Dieser Datensatz ist für das Modell noch unbekannt. Mit dem Datesatz`datasetTest` stellte ich fest, dass unser Forest allgemein 13 Männer falsch klassifiziert hat. Dies entspricht der Realität und wird auch als **Generalisierungsfehler** benennant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.87"
      ],
      "text/latex": [
       "0.87"
      ],
      "text/markdown": [
       "0.87"
      ],
      "text/plain": [
       "[1] 0.87"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(tab[1,1]+tab[2,2])/length(wahreKlasseTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier seht man den Anteil aller Männer die eine richtige Vorhersage gekriegt haben. Das nennt man **Accuracy** . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>MeanDecreaseAccuracy</th><th scope=col>MeanDecreaseGini</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>altersunterschied</th><td> 2.4217766</td><td> 4.9210198</td></tr>\n",
       "\t<tr><th scope=row>iq</th><td> 6.8981187</td><td>10.7817952</td></tr>\n",
       "\t<tr><th scope=row>konsumierte_bierglaeser</th><td> 9.7809517</td><td>11.3687139</td></tr>\n",
       "\t<tr><th scope=row>aufenthaltsdauer</th><td> 8.7387670</td><td>14.5600578</td></tr>\n",
       "\t<tr><th scope=row>haarfarbe</th><td> 2.6122170</td><td> 4.9311040</td></tr>\n",
       "\t<tr><th scope=row>hat_Freundin</th><td>-0.2369585</td><td> 0.9280425</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & MeanDecreaseAccuracy & MeanDecreaseGini\\\\\n",
       "\\hline\n",
       "\taltersunterschied &  2.4217766 &  4.9210198\\\\\n",
       "\tiq &  6.8981187 & 10.7817952\\\\\n",
       "\tkonsumierte\\_bierglaeser &  9.7809517 & 11.3687139\\\\\n",
       "\taufenthaltsdauer &  8.7387670 & 14.5600578\\\\\n",
       "\thaarfarbe &  2.6122170 &  4.9311040\\\\\n",
       "\that\\_Freundin & -0.2369585 &  0.9280425\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | MeanDecreaseAccuracy | MeanDecreaseGini | \n",
       "|---|---|---|---|---|---|\n",
       "| altersunterschied |  2.4217766 |  4.9210198 | \n",
       "| iq |  6.8981187 | 10.7817952 | \n",
       "| konsumierte_bierglaeser |  9.7809517 | 11.3687139 | \n",
       "| aufenthaltsdauer |  8.7387670 | 14.5600578 | \n",
       "| haarfarbe |  2.6122170 |  4.9311040 | \n",
       "| hat_Freundin | -0.2369585 |  0.9280425 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "                        MeanDecreaseAccuracy MeanDecreaseGini\n",
       "altersunterschied        2.4217766            4.9210198      \n",
       "iq                       6.8981187           10.7817952      \n",
       "konsumierte_bierglaeser  9.7809517           11.3687139      \n",
       "aufenthaltsdauer         8.7387670           14.5600578      \n",
       "haarfarbe                2.6122170            4.9311040      \n",
       "hat_Freundin            -0.2369585            0.9280425      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb2+vr7Hx8fQ0NDZ2dnh4eHp6enw8PD////ojgWfAAAACXBIWXMAABJ0\nAAASdAHeZh94AAAgAElEQVR4nO2diZaaWhBFL87Pkf//2sfMBUEL+9qbSp+9VjqG6eCunBa1\nY0IuhPgxgT4BIf4FVCQhEqAiCZEAFUmIBKhIQiRARRIiASqSEAlQkYRIgIokRAJUJCESoCIJ\nkQAVSYgEqEhCJEBFEiIBKpIQCVCRhEiAiiREAlQkIRKgIgmRABVJiASoSEIkQEUSIgEqkhAJ\nUJGESICKJEQCVCQhEqAieSC0bE+jBQXsqYkajcEDUW224wWa4CrQGDzQteWShcNggVgJGogH\n+t7cwma4QKwDDWQNFL04b8LmkuenLGzOxZJ7XZj8EbJ80Jv6poq0NjSQNRDCsXq6cz3UvxWL\nNuFerjlXl3J6RFo9GsgaCCErHo32Iat/K19QOIbygSnfhkve9+ZRPEc65bmKtD40kDVQPwg9\nQtWaR9WSe/36XHslN/+qHXXOYoDmsAaaOgx+25RfL22dGnaXdgcVaV1oDmtgqkjHUF3mRVdy\nh7B9DHcQq0EDWQNTRbqHfXnzHq3YNBd2KtL60EDWwFSRytftbtWL392SWwjHwQ5iNWgga2Cy\nSOdwuNU/x9D15hjCbbBArAUNZA1MFukRQv3id9SbLOyGC8RK0EDWwGSR8m33mlz/s3b1K+Qq\n0urQQNbAdJHOoXn8iXqzq581qUhrQwNZA9NFKq7tzoP1eflaXvWCuIq0NjSQ9fJoXvwWDlCR\n1su5vbIT60dFWi2PTfNat3CAirRWQih/tEE4QUVaK5vmzVjhAhVJiASoSEIkQEUSIgEqkhAJ\nUJGESICKJEQCVCQhEqAiCZEAFUmIBKhIQiRARRIiASqSEAlQkYRIgIokRAJUJCESoCIJkQAV\nSYgEqEhCJEBFEiIBKpIQCVCRhEiAiiREAlQkIRKgIgmRABVJiASoSEIkQEUSIgEqkhAJUJGE\nSICKJEQCVCQhEqAiCZEAFUmIBKhIQiRARRIiASqSEAlQkYRIgIokRAJUJCESoCIJkQAVSYgE\nqEhCJEBFEiIBKpIQCVCRhEiAiiREAlQkIRKgIgmRABVJiASoSEIkQEUSIgEqkhAJUJGESICK\nJEQCVCQhEqAiCZEAFUmIBKhIQiRARRIiASqSEAlQkYRIgIokRAJUJCESoCIJkQAVSYgEqEhC\nJEBFEiIBKpIQCVCRhEiAiiREAlQkIRKgIgmRABVJiASoSEIkQEUSIgEqkhAJUJGESICKJEQC\nVCQhEqAiCZEAFUmIBKhIQiRARRIiASqSEAlQkYRIgIokRAJUJCESoCIJkQAVSYgEqEhCJEBF\nEiIBKpIQCVCRhEiAiiREAlQkIRKgIvUEGlqAA+gRzc5Iw+uhXdD5HqAdqUgGaBd0vgdoRyqS\nAdoFne8B2pGKZIB2Qed7gHakIhmgXdD5HqAdqUgGaBd0vgdoRyqSAdoFne8B2pGKZIB2Qed7\ngHakIhmgXdD5HqAdqUgGaBd0vgdoRyqSAdoFne8B2pGKZIB2Qed7gHakIhmgXdD5HqAdqUgG\naBd0vgdoRyqSAdoFne8B2pGKZIB2Qed7gHakIhmgXdD5HqAdqUgGaBd0vgdoRyqSAdoFne8B\n2pGKZIB2Qed7gHakIhmgXdD5HqAdqUgGaBd0vgdoRyqSAdoFne8B2pGKZIB2Qed7gHakIhmg\nXdD5HqAdqUgGaBd0vgdoRyqSAdoFne8B2pGKZIB2Qed7gHakIhmgXdD5HqAdqUgGaBd0vgdo\nRyqSAdoFne8B2pGKZIB2Qed7gHakIhmgXdD5HqAdqUgGaBd0vgdoRyqSAdoFne8B2pGKZIB2\nQed7gHakIhmgXdD5HqAdqUgGaBd0vgdoRyqSAdoFne8B2pGKZIB2Qed7gHakIhmgXdD5HqAd\nqUgGaBd0vgdoRyqSAdoFne8B2pGKZIB2Qed7gHakIhmgXdD5HqAdqUgGaBd0vgdoRyqSAdoF\nne8B2pGKZIB2Qed7gHakIhmgXdD5HqAdqUgGaBd0vgdoRyqSAdoFne8B2pGKZIB2Qed7gHak\nIhmgXdD5HqAdqUgGaBd0vgdoRyqSAdoFne8B2pGKZIB2Qed7gHakIhmgXdD5HqAdqUgGaBd0\nvgdoRyqSAdoFne8B2pGKZIB2Qed7gHakIhmgXdD5HqAdqUgGaBd0vgdoRyqSAdoFne8B2pGK\nZIB2Qed7gHakIhmgXdD5HqAdqUgGaBd0vgdoRyqSAdoFne8B2pGKZIB2Qed7gHakIhmgXdD5\nHqAdqUgGaBd0vgdoRyqSAdoFne8B2pGKZIB2Qed7gHakIhmgXdD5HqAdqUgGaBd0vgdoRyqS\nAdoFne8B2pGKZIB2Qed7gHakIhmgXdD5HqAdqUgGaBd0vgdoR8uLFH5+yjOHuJj2eJuf4ATH\nR0x9QCb/sQ/hML2qUj8prlo4PZn0on8AfS4rKtLmxYFVpATsQgjHyTW1+tkizUxGRbLkf7NI\nyw+8JPSfKlIo702i/BDus2v6rxOrZoyqSE30yxmpSNERUx/QHBzyZk7JjvZijYr0UfCbGb2R\nfqguEk6bsDnVC++7kFXXDZdtCNtLHk+n+HWs1h7qi/R6TbFz1uz82ITyumO0fBh6CNkhn9u5\nPKOsOHY39ktxvKx5QtAfsDu3qQO8cPVy7Repgz/4C9vf/W4KoRUc3fNmaM2aWvLxaf92x8he\nJ3sqKV9mNw2rndHrIh3Ctvi6rRRvq4VZqK/AT/XATsMiHctll2r7Q7NmF+1c3D604+qXD0J3\n/dYTOzfnsm+LdKzPohpwv21/bhMHeOvq9wmj381Ed/+pSPE9b4bWFWnX6Bnu36yO7FWydyPR\nUZEW2U3Damf0skh1j84hu+W3LJzLhdtHYXqT51m4lWs2wyJVa+uvWb3sUv7psQ2XZnW7fbR8\nEBpFTe18aTYIbeK5PIswPGB/bs8HeCnrvzz/D/j1cZGiux/99R7r6IfWbtT/ebB/vbq3d45l\nTyQts5uG1c7oVZHqHhXfd8q/7ZfyDyFc81b6pd+wW1avvef9YHahFPwoH/br1c320fJBaB21\nm9u5PZfBFX3z3THatj235wO8HNK7Db7Ex0Vqdpsq0tM975X1fx7t367u7ZUbXia2nBvP91nt\njF4UaTv4mz9QXT0N2t1u/erh2v7PoWVwnMHyQWhu2TlKu1+O2/Z5YLttfG7jA1hk/TofP0eK\n7353iLGO+QGN9m+fFg8nO7Xl3Hi+z2pn9KJIxaX1Jtp5NIljeeGd3eEibbuDxAeMzs1HkZo7\nsHjH+O5HB8qtRRru35zAaLJTW/7FIr2Z0YsiXW/VlfFMkYrH/MNm/BypX9t3YXgqo2NMne37\nnbuV+7A5Xe7RfFsG5xbv+QpsSM2EFuc/3f24SNGx26/jAY327/YZ2psUPT2e77PaGb0oUvla\nTZb3z0t2zyXodF7nirTrX04YdGEXpn4eJTRX5bu5nUfPkaql9XyfDjh9gFeAQ/osP7r7/RTy\nsY7X3+nyiSLlsb3rxJb1wmV207DaGb0sUr4pX+k+D19Kq79u6pdxNuWtU/nSzcycqp3zU9/C\nvH4xIlo+CK2jLnM7P71qd81v9aV7tG1/bs8H+EjSL/FBkbq7308hH+uIB9K+DvSkr1/d2xvI\nnkhaZjcNq53R6yIVF3eP4ftI7apzfXV8bd532M1+w6t3ji65N6F6nOuXD0L31dHyuZ3ba/Xm\n6IfQnUa07Xlq4b9YpOju91No7un4ntff/dp3JZ709asje9X7RPvRllHSIrtpWO2MXhepuLgr\n/1Kfsu4nG7qv1fvf1at6xZPT/fyVQ/nud9j33wnz66YqUr98GNq+6T69c/Vm+7a/2tiXJ3Gp\nH9f6A/bnNnGADyT9Esvzo7vfTaG9p6N7Xn6t1Udj6vfvV8f2jt1PNkwmLbKbhtXOiD6xzxj/\nRESio37joI7yPUA7+leKVL2O+Nh956dRaBd0vgdoRysuUuh5v3HzM1/Zd87kK0f1k+8B2tG/\nUqT8VFzAb77005G0CzrfA7SjFRdpPdAu6HwP0I5UJAO0CzrfA7QjFckA7YLO9wDtSEUyQLug\n8z1AO1KRDNAu6HwP0I5UJAO0CzrfA7QjFckA7YLO9wDtSEUyQLug8z1AO1KRDNAu6HwP0I5U\nJAO0CzrfA7QjFckA7YLO9wDtSEUyQLug8z1AO1KRDNAu6HwP0I5UJAO0CzrfA7QjFckA7YLO\n9wDtSEUyQLug8z1AO1KRDNAu6HwP0I5UJAO0CzrfA7QjFckA7YLO9wDtSEUyQLug8z1AO1KR\nDNAu6HwP0I5UJAO0CzrfA7QjFckA7YLO9wDtSEUyQLug8z1AO1KRDNAu6HwP0I5UJAO0Czrf\nA7QjFckA7YLO9wDtSEUyQLug8z1AO1KRDNAu6HwP0I5UJAO0CzrfA7QjFckA7YLO9wDtSEUy\nQLug8z1AO1KRDNAu6HwP0I5UJAO0CzrfA7QjFckA7YLO9wDtSEUyQLug8z1AO1KRDNAu6HwP\n0I5UJAO0CzrfA7QjFckA7YLO9wDtSEUyQLug8z1AO1KRDNAu6HwP0I5UJAO0CzrfA7QjFckA\n7YLO9wDtSEUyQLug8z1AO1KRDNAu6HwP0I5UJAO0CzrfA7QjFckA7YLO9wDtSEUyQLug8z1A\nO1KRDNAu6HwP0I5UJAO0CzrfA7QjFckA7YLO9wDtSEUyQLug8z1AO1KRDNAu6HwP0I5UJAO0\nCzrfA7QjFckA7YLO9wDtSEUyQLug8z1AO1KRDNAu6HwP0I5UJAO0CzrfA7QjFckA7YLO9wDt\nSEUyQLug8z1AO1KRDNAu6HwP0I5UJAO0CzrfA7QjFckA7YLO9wDtSEUyQLug8z1AO1KRDNAu\n6HwP0I5UJAO0CzrfA7QjFckA7YLO9wDtKFWRHvsQDtOrLtXhpo5XLbxMx9NiYuhzWZyfwN7M\nIaanNd7jbX768a52RgtPbBdCOE6u2VRHmi3SZjpIRfpJ/teKNDOt8R4qkmHFzObhPrum/zqx\nakbpHy5SeLrvRJGWH3hJqMMiPY/Flr+4SK/XqEjWrJA/3XsVyXDE1AccHX5iLLb8+X0uxWVc\nduiOXn4NFcXt0yZkp3rhfReyY543a4pfh+rPo/3bHS/bELb1BfghK55t1Qufk4YZj03YWTT8\nkF8t0kTih0U6VBfbha7NcCSR7d5r8etYrT3UT3YnXbfT6pcPQw+DcT0PqhvtaLzxAaO/CUsm\n/e0ivQtZXqRj3ZpedlSk2vS2WpiF6mlTV6Rq3Wm8f7P6VC8srW3LG7ty4VTSMGM3+wpHUn6x\nSGH0+2f5lahDJWn7PJLIdlykSvdl2yifdN0WqV8+CN31W0/s3JzLvi1SNN5o2+hvwqJJf3dG\nk2Ox5b+4VDvn+Tl6qOtvXsL2kT+24VL+ubh5Cpt+o/7Pg/3r1Vm4lQs35Zfslt+yetVE0jjj\nNwj/5fl/v/Orm1i8/KMi1T1qdZ7jEfS24yJVa+uv2Zzrevto+SA0ipra+RKPdjDeaNv+3JZN\n+rsz6os0t82nz5Emi7QL5d19lA/CIVzzuCn9n0f7t6vboeyqDS8TW9ZfnzJ+AY+PSHWPCl2l\n2Ev5h3gkl37Dblm99p73c3l2XW8fLR+E1lG7uZ3bcwlPfxEG2/Z/E5ZM2t8jUp7fL8ftZJFC\ny+jie7jpcP/2cj7sbrd+k6kt2+1HGb+Aw+dI28Hf/NEgxranxjTtul3fLR+E5pado7RovN22\n8bktmfSXZzQ1Flv+/D7bTuJnRRru3zg6ltfv2X0ofSrpny9Sf3c/zy9dlldHM0Ua24aKFI+3\nP2B0bmsq0sRYbPmzK/Zhc7rcZ4o0DJ6c0Gj/bp/LYdNftU9tOdr+Hy1S9Rfop/nFw9Gtegoy\nU6SR7fkiRUfMh+unQgdRczt3K5/G2zL8mzAfOEx/u8UPeR6LLX9+Rbkmuv/X/uauf/r5ekL5\nRJGaP9fHuE5sWS98yvgFfrdICfJLM8fyJYPuecnuuQSd4uvcmJ5d17/twtRPCoXm2e1ubufR\nc6RovE8HXD7p1c7oRZGu+a2+tN2EU/mySvc3vXqJKD8Np9Y/f23/3O3fr97Ur+BsRi/tTCQ9\nZfwCqx3S7A61s+PoVbt2VW+79zpVpGfX9bSi5YPQOuoyt/PTq3bdeKNt+3NbNunVzmh2xaG5\ncr02r/nvogfn+qp3ePG9Ce2rqfXXaP9+9blbVr97sB9tGSWNM36B1Q5pdodyj+Li7jF8H6ld\ndZ7wOnnh8OS6nla0fBC6r46Wz+3cPikKT38Rom3PUwv/ySIV17Zhe60ewcsnhvvB06NTYXo/\nfAS6boZFivbvV9fvZ9evcR67n2yYTBpn/AKrHdLsDtUex8rcKet+sqH72tvuvE4W6cl1M61+\n+TC0/eGVuUEdsiK2O3o03uiA0d+EJZNe7YzoE1sTtAs6Py3jn4hIdNRvHDRFPn1ia4J2Qecn\nonod8bH7zk910Y5UJAO0Czp/ntDzfuPmh+uy75zJV46aIJ8+sTVBu6Dz51lUpPxUPP3ZfOmn\njGlHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpk\ngHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0Pke\noB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB\n2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uA\ndqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZo\nF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHa\nkYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd\n0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hH\nKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB\n53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2p\nSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd\n7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQi\nGaBd0PkeoB0tL1KgT/n3oe/x4nzNaD35KlIPfY9VpPfQ91iXdga+5iLY/sZrFu/5piPLmFQk\nA19yUY3HUiXN4j3fc2Qb06eXdocsHP/MFcS3imQ9+IeXdppRwiN/qUjb4tEuHDWkFEf9wWXD\n7A6aUfIDvwn4rEjnkN3yW/ZnhvRfnv+X/Fc/oXfbflQkzSjNr6hIH83oZZF24VLcuvyZIX31\nqF96RNKM0h74K49IzXQ0pASH/dZzJM0o6ZG/8xxJQ0py2K+9aqcZpTzyF1+105ASHfhL7yNp\nRmmP/ZX3kfrr76uGtNJ8zWg9+S+LdPljrwh5y9eM1pP/+n2kQ/kexVZDWmm+ZrSe/Dc/2XDa\nhJ2uv9earxmtJ99yYhrS+vM1IzhfReqh76aK9B76bqpIBui7qSK9h76bKpIB+m6qSO+h7+aP\nivRXoF3Q+R6gHalIBmgXdL4HaEcqkgHaBZ3vAdqRimSAdkHne4B2pCIZoF3Q+R6gHalIBmgX\ndL4HaEcqkgHaBZ3vAdqRimSAdkHne4B2pCIZoF3Q+R6gHalIBmgXdL4HaEcqkgHaBZ3vAdqR\nimSAdkHne4B2pCIZoF3Q+R6gHalIBmgXdL4HaEcqkgHaBZ3vAdqRimSAdkHne4B2pCIZoF3Q\n+R6gHalIBmgXdL4HaEcqkgHaBZ3vAdqRimSAdkHne4B2pCIZoF3Q+R6gHalIBmgXdL4HaEcq\nkgHaBZ3vAdqRimSAdkHne4B2pCIZoF3Q+R6gHalIBmgXdL4HaEcqkgHaBZ3vAdqRimSAdkHn\ne4B2pCIZoF3Q+R6gHalIBmgXdL4HaEcqkgHaBZ3vAdqRimSAdkHne4B2pCIZoF3Q+R6gHalI\nBmgXdL4HaEcqkgHaBZ3vAdqRimSAdkHne4B2pCIZoF3Q+R6gHalIBmgXdL4HaEcqkgHaBZ3v\nAdqRimSAdkHne4B2pCIZoF3Q+R6gHalIBmgXdL4HaEcqkgHaBZ3vAdqRimSAdkHne4B2pCIZ\noF3Q+R6gHalIBmgXdL4HaEcqkgHaBZ3vAdqRimSAdkHne4B2pCIZoF3Q+R6gHalIBmgXdL4H\naEcqkgHaBZ3vAdqRimSAdkHne4B2pCIZoF3Q+R6gHalIBmgXdL4HaEcqkgHaBZ3vAdqRimSA\ndkHne4B2pCIZoF3Q+R6gHalIBmgXdL4HaEcqkgHaBZ3vAdqRimSAdkHne4B2pCIZoF3Q+R6g\nHalIBmgXdL4HaEcqkgHaBZ3vAdqRimSAdkHne4B2pCIZoF3Q+R6gHalIBmgXdL4HaEfLixSM\np/zYh3D4fPc1QZ/y4nzNaD35Py7SLoRw/Hz3NUGf8teKpBl9P//HRQrh/pPd14TtlMPX7trX\nivTnZtRs+4U7+M0i/Wj3NWE55ep+fenOfbFIP9p9TdhP+TuT+qhIh5BVFwSX4tIgq6+x+5sh\nPDahvGaoej9e3uzeXJifNiE7JbsvX8NUJPOW38kf7qAZvdtyDUWqJlC4PdaTKIVHN6v1h2ZI\nT8vb3bfloepRbhPfpfQYpIfR77+dP9pBM3q7YeJJfVKk7SM/hU1565zn5+aRMrpZrM+7R9Dx\n8uKb3y2/ZeXyS7nksQ2XtHcpPeG/PP/v9a9uPG+2++jXB0XSjOZ+RUX6lRm9KNI1j68xo6vN\nZhjXV8tDNZNLcQFRfLMrp/Yob64bh49ImtG7DVfwiNR/vV+O22YY3c12ODPLo99Cy0/vxLfx\n+Bwp14xebrmG50jd123vuL85HNLT8n91SCt81U4zmt5yPa/aNV/3YXO63Ks/RzcHQ3pePhhS\nmvvwdWznubr3kTSjuW1X9D5SJzmawMSQnpfXl+HN9ffqn8LW0H+ZflAkzYjONxXpmt/ai+vo\nZrTd8/L2FaFL+UpRcTM//UNPZNeSrxmtJ99QpOaNiPLbV3RzMKTn5SHsywXVYOqL82zyx1TW\nxGqHNLuDZrSafMuLDYXu7bW6AohuDp/IPi0P/Zvu1bvmYb/6Ga13SLM7aEaryadPbE3QLuh8\nD9COVCQDtAs63wO0IxXJAO2CzvcA7UhFMkC7oPM9QDtSkQzQLuh8D9COVCQDtAs63wO0IxXJ\nAO2CzvcA7UhFMkC7oPM9QDtSkQzQLuh8D9COVCQDtAs63wO0IxXJAO2CzvcA7UhFMkC7oPM9\nQDtSkQzQLuh8D9COVCQDtAs63wO0IxXJAO2CzvcA7UhFMkC7oPM9QDtSkQzQLuh8D9COVCQD\ntAs63wO0IxXJAO2CzvcA7UhFMkC7oPM9QDtSkQzQLuh8D9COVCQDtAs63wO0IxXJAO2CzvcA\n7UhFMkC7oPM9QDtSkQzQLuh8D9COVCQDtAs63wO0IxXJAO2CzvcA7UhFMkC7oPM9QDtSkQzQ\nLuh8D9COVCQDtAs63wO0IxXJAO2CzvcA7UhFMkC7oPM9QDtSkQzQLuh8D9COVCQDtAs63wO0\nIxXJAO2CzvcA7UhFMkC7oPM9QDtSkQzQLuh8D9COVCQDtAs63wO0IxXJAO2CzvcA7UhFMkC7\noPM9QDtSkQzQLuh8D9COVCQDtAs63wO0IxXJAO2CzvcA7UhFMkC7oPM9QDtSkQzQLuh8D9CO\nVCQDtAs63wO0IxXJAO2CzvcA7UhFMkC7oPM9QDtSkQzQLuh8D9COVCQDtAs63wO0IxXJAO2C\nzvcA7UhFMkC7oPM9QDtSkQzQLuh8D9COVCQDtAs63wO0IxXJAO2CzvcA7UhFMkC7oPM9QDtS\nkQzQLuh8D9COVCQDtAs63wO0IxXJAO2CzvcA7UhFMkC7oPM9QDtSkQzQLuh8D9COVCQDtAs6\n3wO0IxXJAO2CzvcA7UhFMkC7oPM9QDtSkQzQLuh8D9COVCQDtAs63wO0IxXJAO2CzvcA7UhF\nMkC7oPM9QDtSkQzQLuh8D9COVCQDtAs63wO0IxXJAO2CzvcA7ehHRQrlRpePchfsFcLzren1\n32K1Q7Lsqxn9Cj8u0uaje7BkLw3pp0XSjL7Pj4v0maIle73bdhVDCt88i58WSTMabfuF01GR\nLGfwdoPw1fNQkQxnYN/yK8P6oEiXXQjZoTmlEJp6nzYhO9ULH5uwKzbbhrC95O051wO970J2\nzC175fkhC9t7veeh2isf7VNtcljDkIJts6/lP6EZvduSL9KxMhzKKUVD2lU3ttXCXbn2VG92\nGg4pK5cdLXvl2/JG9mhW9Ufq96k32fFDCqPffzv/Cc3o/YaJz2h5kUI45/m5e3ysFV3C9pE/\ntuFSLihu5nkWbuVmm+GQilWnftmrvc7lgn35d2G4V7TPOWS3/Jb9wpD+y/P/XvwKxu0+/bX4\nDmpG87+iIv3KjCwXvf2QdqFU/Cgf+EO41htc4k2bba/mvXblgkfIxntF+1SbFFPDv9ut7hGp\n2U0zerEh/oiU5/fLcTsaUmjpnsYdigf02y3Ph0Ma33q31/P+z/usYEire46kGb3bkn+OVF/1\nvh1SfiyvtbP7siGN92q3X/mQvvNCkD3/Cc3oxZZfGdbyIu3D5nS5Pw2p37G7eTlsxtffz7de\n79Wv6oc03mcVQwrfPIvFh9aMXm/7hdNZXqTqJMZD2nXXzUNn3RbXqSG92msbXX/He0X71Dev\n63bR284AAA+gSURBVBrSKvI1o1/nkyJd81t8/V2+j1C9OJOf6qek1Wab+oWjTXnrVL6EMxrS\nu71O5Ss/h/oVoX6vwT6XX3tF6NsBqfM1o19neZEOzfXvtZa2CeV3pOaiPLpwPndbnbp3Enrd\n7/eK36No96pu9fvUb1fs//CQ5tCMfp3lRSouwMP2emm/RV03le7yveywb97krqje/65eLy2e\nnO5HV92GvapXh/pV/f7dPtWR1/Wu+VryNaPf5oMi/T1oF3S+B2hHKpIB2gWd7wHakYpkgHZB\n53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2p\nSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd\n7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQi\nGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+\nB2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpk\ngHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0Pke\noB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB\n2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uA\ndqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZo\nF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHa\nkYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd\n0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hH\nKpIB2gWd7wHakYpkgHZB53uAdrS8SGFyzeV5s4alJxTmIjjo01mcrxmtJ39ZkTZPCzUkMF8z\nWk/+siI9L/xU9NoGVFKe0vK/bmnzl+3wN2fU3iTOT0UyEOqzwk5NRXpPd0rQqD4q0iFkx/Lm\nZRdCdsiba4Snzfqbj03YFTdOm5Cd+pX1FcJ91xwtP2Th0F02xCtoQmPDU5H+3owGN1wUaVeO\npJB9rC+wD++HtKu2qvYL23w4pKxcWA5jW97Y9UPqVuB09wVq0gdF+nszmvn9t/MXrAjbR34K\nm/LWOc/PMw+l0fPYao/ie2P522MbLsMhtUc7h+yW37J+SO0KnrhI//33+78+KNLfm9HIVVjL\njF4U6ZrHU7EMqdyj+GZXjupRXkDEQ2qPtqtuXfohXSePS+DwEenvzWjm99/OX7Ai9F/vl+N2\ndkjjm9GLrfGQhrei6+/RUUA8PkfK/9yMBjdcPEfqvm6jb2jTm8U3PQ/J5at2f2xG7Q1mVD8p\n0j5sTpf7giFNHMPNkBy+j/TnZtTeJM7oJ0WqbtmHtAuXwZLrcBb16uvKh+QjXzNaT76pSNf8\n1l5/3yc3i29WL/nkp/KJ7CacyteGBrO4jF8RGh0FhD6JHxVJM2LzDUU6NNfT19J6yKY2G9ys\nL9ezezmp+O2I5mv1FsZeQ0qQrxmtJ99QpOICPGyvl/Lb13VjGFL5rnnYV98Wj1nYP83iOHzX\nfHQUEPokflAkzYjOp09sTdAu6HwP0I5UJAO0CzrfA7SjlEUKPT85pfVB352E+ZrRb+erSD30\n3VGR3kPfHV3aGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0Pke\noB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB\n2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uA\ndqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZo\nF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHa\nkYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd\n0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hH\nKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB\n53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2p\nSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd\n7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQi\nGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+\nB2hHKpIB2gWd7wHakYpkgHZB53uAdqQiGaBd0PkeoB2pSAZoF3S+B2hHKpIB2gWd7wHakYpk\ngHZB53uAdqQiGQg0tAAH0CNSkRYw42ROVaLlmoQZs6rkG85vp/E9oyKtHBXJByrSylGRfKAi\nrRwVyQcq0spRkXygIq0cFckHKtLKUZF8oCKtHBXJByrSylGRfKAirRwVyQcq0spRkXygIq0c\nFUmIfxQVSYgEqEhCJEBFEiIBKpIQCVCRhEiAiiREAlQkIRKgIgmRABVJiASoSEIkQEUSIgEq\n0ohDFrLDY8ke1wUOH8sPL4ZYDb75QMeWU7vFm+O2280dVkUasq00bRbs8cjsDu9ZdfjsvvzE\nRI11QDdbkW7tFm+O2243e1gVacA1ZLf8loWrfZfdgo8a3odD8fUQ9svPTFSYB3QLO8PhiiMF\ny3G77WYPqyINOIRL8fUcjuY9zobLh45mU33K98eYB3SyzPAUts0sXh+33272sBrpgF0or7ps\n380q7p1iC81VYMgWn5ioMQ/oFE7vj1ZcIDTTe33cfrvZw6pIAxY/ZGzDfUGRjs2lnf0BTwwx\nD2gXLvuQHV5vdBsfcOa4/Xazh1WRBiwt0jGcF12oncpXGzLD90oxzYIiVWyXHfDFf9vSFmnm\nsCrSgIVFqi4FlhTpWI1BD0gfYx5QKL7F5Y/D2wu8hUWaPayKNGBhkTbZY1GRTuWl3WNvuXwX\nkywc0OPtC+ULizR7WBVpQLZoTvvqpZ4FRdqE8h2/99MVcywbkGHDZoO3xx2ued5ORRpQv3hz\nN75q9/4/RHzaIf5NLGfZgOxFentcFWkRx+ox5hLevNrTsLhI9fe9h17+/hjzgLLqwf9945rZ\nvT1u98g1c1gVacAHP9mw5PHlEMqf5joYeyqeMQ+okvyo32d9he0nG7rtZg+rIg3Z2F40jVly\nobZdfngxwDqgR/1jjW+/ZbXTe3fcZrvZw6pIQ+ofz160y6JnPMsPLwaYB1RuuHn/8mg7vXfH\njbebOqyKJEQCVCQhEqAiCZEAFUmIBKhIQiRARRIiASqSEAlQkYRIgIokRAJUJCESoCIJkQAV\nSYgEqEhCJEBFEiIB/36Rst2p/qjt+2ln+pep9b953ST/qPtM/zB2KWuZ3eNUfgzX7tRkDBOb\n39MmrpDCQP1R23vjPwlv//l44o+6vxSHfPfPNcWQlczuksWH/btF2tTfzLKNdRjl1/s28T9k\n3Qd9eP5S1jG74lvgvvwn6Nfdi2uKv1CkQ7jl5Yc5HpYMo/zHx0kfQIohLPgPYETJOmaXdQd7\n8YmE//5oiwuq6u6fwrn9IPRN+6nBl+LSt/73xSHci+83x/pmveOlfgDpty7/nfi2fnB/bKrP\nkZk6Un7ZhrC9jJLyczjkh/JjOuMDxYdso9uDR0f8s6xiduf+I4Puh2s7pDbyL13aPSoTu/bj\n7nfdx1zUHyBcfZJFobL9MOHuX+dXn+PYb11/ckn92aq7aq/JI53qm6fhvsXO1/za3W4OFB8y\nz9sZVQePjvh3WcXsdqOPFqqH1Eb+pSIVD/R5dWlV3elL2D7yx7Z8uK4+yLn+XhfKpadKf3cR\nUd6Itj6XN6v/KqzaeO5IWXkxci6PFK1vPsuu/lS0/kDxIdvE5uDREc+/rWw1rGJ244vKbkhV\n5J8q0qF6NNjXd3rXfGzwLtqg/HLtb0Yroq2r70xVIeqNZ4/UXlHH68/Nf+hyzuMDxYdsDxDi\nb4D1GX/Big9WMbvu43GbjwLth9T+qVr9NQlrobin5X/E1v0HLINPR71fjttoAs/DiLbuvzMN\n1Y6OVDwt3t1u4/Wbyvxt+G3z6dYgpj+35E68sIrZTRepD2mO+2UVPMU9vRfXuttwfx7Gtrs1\nMYxyL/Mw+iPlx+r/QLoP1t+723drkaJz+5qbtbOK2cXPkf54kcrnJt31U3TJuw+b0+U+N4xz\n/9ylO9Dg1vSRCi6HTXz5nHdPaMPg+enLIsXn9jMBjlnF7M7R239/vUjNm6HNdfYlXpXPDqO6\nGou23o6e0MwcqV8Qrd+E5kddyiH1Bxod8jq4kuiO+Hc/4XgVs4veR3r89SKV//X4ubnT5/Lj\n0vNT/TTymt9mrrObd8ejrU/lKzWH6Fvd9JE29YtAm3h9/5/8bsMtOlB/axNO5YtEcZG6I/7d\n/5ZsDbOrfrJhV/1kQ/EUKvvbRbrHT07qS+LyQvjQXG5dR8MI3Rbx1uM3fWaOdO5u9ev7/72g\n+o9DJt5Hqt7B2EWTiY74d1nD7Aqu7c/aVRd5f7lIzU9eN3f6tCmMVIb2IWyvl/41zmgY2/Y/\neu23rl7UGfzY4tSR6nfHr4P1Wf8zWtXN9kDRreJp7n4wmeiIf5Y1zK7ivMvKw97boL9YJCF+\nARVJiASoSEIkQEUSIgEqkhAJUJGESICKJEQCVCQhEqAiCZEAFUmIBKhIQiRARRIiASqSEAlQ\nkYRIgIokRAJUJCESoCIJkQAVSYgEqEhCJEBFEiIBKpIQCVCRhEiAiiREAlQkIRKgIgmRABVJ\niASoSEIkQEUSIgH/A/GR6xbtGWSIAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imp <- varImpPlot(myRF)\n",
    "imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der Methode Variable Importance Plot `varImpPlot` kann man die **Variablenwichtigkeit** betrachten. Man seht in der Tabelle oben die 7 Variablen mit deren Wichtigkeit in Zahlen und mit deren Hilfe das Plot erstellt wird. Ganz oben steht die wichtigste Variable und ganz unten die unwichtigstee Variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geschafft !!!!\n",
    "![Geschafft!!!](img/check.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Machine Learning with Python <br>\n",
    "Solve challenging data science problems by mastering cutting-edge machine learning techniques in Python <br>\n",
    "John Hearty<br>\n",
    "Kapitel 8 Ensemble Methods\n",
    "\n",
    "Ensemble Models - Boosting, Bagging and Stacking, Maximilian Schwinger, 3. Februar 2004\n",
    "http://campar.in.tum.de/twiki/pub/Far/MachineLearningWiSe2003/schwingerm_ausarbeitung.pdf\n",
    "\n",
    "Bootstrap aggregating bagging\n",
    "https://www.youtube.com/watch?v=2Mg8QD0F1dQ\n",
    "\n",
    "Einfache Einführung in den Random Forest Algorithmus zur Klassifikation\n",
    "https://www.youtube.com/watch?v=gksq_QaJ34w\n",
    "\n",
    "Ensemble Methoden zur automatisierten Klassifikation von NMR-Spektren\n",
    "http://patrec.cs.tu-dortmund.de/pubs/theses/diplomarbeit_Jana_Ehlers.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
